{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":61542,"databundleVersionId":7516023,"sourceType":"competition"},{"sourceId":6847931,"sourceType":"datasetVersion","datasetId":3936750},{"sourceId":7432540,"sourceType":"datasetVersion","datasetId":4325258},{"sourceId":7452416,"sourceType":"datasetVersion","datasetId":4336615}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <b><span style='color:#F1A424'>|</span> Import Libraries</b><a class='anchor' id='import_libraries'></a> [↑](#top) \n\n***\n\nImport all the required libraries for this notebook.","metadata":{}},{"cell_type":"code","source":"!pip install causal-conv1d>=1.4.0\n!pip install mamba-ssm","metadata":{"execution":{"iopub.status.busy":"2024-09-19T17:04:21.341659Z","iopub.execute_input":"2024-09-19T17:04:21.341975Z","iopub.status.idle":"2024-09-19T17:05:23.517479Z","shell.execute_reply.started":"2024-09-19T17:04:21.341935Z","shell.execute_reply":"2024-09-19T17:05:23.516325Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting mamba-ssm\n  Downloading mamba_ssm-2.2.2.tar.gz (85 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m869.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from mamba-ssm) (2.4.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from mamba-ssm) (21.3)\nRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from mamba-ssm) (1.11.1.1)\nCollecting einops (from mamba-ssm)\n  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\nCollecting triton (from mamba-ssm)\n  Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from mamba-ssm) (4.44.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->mamba-ssm) (3.1.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->mamba-ssm) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->mamba-ssm) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->mamba-ssm) (1.13.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->mamba-ssm) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->mamba-ssm) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->mamba-ssm) (2024.6.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers->mamba-ssm) (0.24.6)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers->mamba-ssm) (1.26.4)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers->mamba-ssm) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->mamba-ssm) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers->mamba-ssm) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers->mamba-ssm) (0.4.4)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers->mamba-ssm) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers->mamba-ssm) (4.66.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->mamba-ssm) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->mamba-ssm) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->mamba-ssm) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->mamba-ssm) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->mamba-ssm) (2024.7.4)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->mamba-ssm) (1.3.0)\nDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: mamba-ssm\n  Building wheel for mamba-ssm (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for mamba-ssm: filename=mamba_ssm-2.2.2-cp310-cp310-linux_x86_64.whl size=323998290 sha256=a658a5438dbe9fb3a53799d7d9f4714ca09225769c24ec04a403728ac7d1c69e\n  Stored in directory: /root/.cache/pip/wheels/57/7c/90/9f963468ecc3791e36e388f9e7b4a4e1e3f90fbb340055aa4d\nSuccessfully built mamba-ssm\nInstalling collected packages: triton, einops, mamba-ssm\nSuccessfully installed einops-0.8.0 mamba-ssm-2.2.2 triton-3.0.0\n","output_type":"stream"}]},{"cell_type":"code","source":"import ast\nimport copy\nimport gc\nimport itertools\nimport joblib\nimport json\nimport math\nimport matplotlib.pyplot as plt\nimport multiprocessing\nimport numpy as np\nimport os\nimport pandas as pd\nimport pickle\nimport random\nimport re\nimport scipy as sp\nimport string\nimport sys\nimport time\nimport warnings\n# import wandb\n\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\nimport torch\nimport torch.nn as nn\nfrom torch.nn import Parameter\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD, AdamW\nfrom torch.optim.lr_scheduler import OneCycleLR\nfrom torch.utils.data import DataLoader, Dataset\nfrom tqdm.auto import tqdm\n\n# ======= OPTIONS =========\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Current device is: {device}\")\nwarnings.filterwarnings(\"ignore\")\n!mkdir output","metadata":{"execution":{"iopub.status.busy":"2024-09-19T17:05:23.519758Z","iopub.execute_input":"2024-09-19T17:05:23.520178Z","iopub.status.idle":"2024-09-19T17:05:27.482622Z","shell.execute_reply.started":"2024-09-19T17:05:23.520134Z","shell.execute_reply":"2024-09-19T17:05:27.481379Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Current device is: cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport random\nimport json\nimport torch\nimport torch.nn as nn\nfrom torch.nn import BCEWithLogitsLoss\nfrom collections import namedtuple\nfrom dataclasses import dataclass, field, asdict\nfrom mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel\nfrom mamba_ssm.utils.hf import load_config_hf, load_state_dict_hf\n# from huggingface_hub import HfApi\n\n# import evaluate\nimport numpy as np\nfrom datasets import load_dataset\nfrom transformers import Trainer\nfrom transformers import DataCollatorWithPadding\nfrom transformers import AutoTokenizer, TrainingArguments\nimport re","metadata":{"execution":{"iopub.status.busy":"2024-09-19T17:05:27.484129Z","iopub.execute_input":"2024-09-19T17:05:27.484828Z","iopub.status.idle":"2024-09-19T17:05:43.959428Z","shell.execute_reply.started":"2024-09-19T17:05:27.484792Z","shell.execute_reply":"2024-09-19T17:05:43.958637Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"import wandb\nfrom huggingface_hub import login\n\nlogin(token=\"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\")\nnotes = \"Train Mamba With 400k row dataset\"","metadata":{"execution":{"iopub.execute_input":"2024-09-18T04:07:22.734063Z","iopub.status.busy":"2024-09-18T04:07:22.733462Z","iopub.status.idle":"2024-09-18T04:07:22.850537Z","shell.execute_reply":"2024-09-18T04:07:22.849644Z","shell.execute_reply.started":"2024-09-18T04:07:22.734029Z"},"trusted":true}},{"cell_type":"markdown","source":"# <b><span style='color:#F1A424'>|</span> Load Data</b><a class='anchor' id='load_data'></a> [↑](#top) \n\n***\n\nLoad data.","metadata":{"papermill":{"duration":0.012589,"end_time":"2022-08-31T07:03:04.13341","exception":false,"start_time":"2022-08-31T07:03:04.120821","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import pandas as pd\nimport re\nimport unicodedata\nfrom tqdm import tqdm\n\n# Load DataFrame\ntrain_df = pd.read_parquet('/kaggle/input/ai-mix-v26/train_essays.parquet')\nvalid_df = pd.read_parquet('/kaggle/input/ai-mix-v26/valid_essays.parquet')\n\n# Define characters to remove\nchar_to_remove = ['{', '£', '\\x97', '¹', 'å', '\\\\', '\\x85', '<', '\\x99', \n                  'é', ']', '+', 'Ö', '\\xa0', '>', '|', '\\x80', '~', '©', \n                  '/', '\\x93', '$', 'Ó', '²', '^', ';', '`', 'á', '*', '(', \n                  '¶', '®', '[', '\\x94', '\\x91', '#', '-', 'ó', ')', '}', '=']\n\n# Define preprocessing function\ndef preprocess_text(text, strategy='light'):    \n    if strategy == \"none\":\n        text = text\n    elif strategy == \"light\":\n        text = text.encode(\"ascii\", \"ignore\").decode('ascii')        \n        text = text.strip()\n        text = text.strip(\"\\\"\")\n        for c in char_to_remove:\n            text = text.replace(c, \"\")\n        if text and text[-1] != \".\":\n            text = text.split(\".\")\n            text = \".\".join(text[:-1])\n            text += \".\"\n    else:\n        text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('ascii')\n        text = text.lower()\n        text = re.sub(r'[^a-z0-9\\s.,;?!:()\\'\\\"%-]', '', text)\n        text = re.sub(r'\\s+', ' ', text).strip()\n    \n    return text\n\n# Apply preprocessing with progress bar\ntqdm.pandas(desc=\"Processing Text\")\ntrain_df['text'] = train_df['text'].progress_apply(lambda x: preprocess_text(x, strategy='light'))\nvalid_df['text'] = valid_df['text'].progress_apply(lambda x: preprocess_text(x, strategy='light'))\n\n# Display the first few rows to verify\nprint(\"Trainging DF Processing\")\nprint(train_df.info())\nprint(\"Testing DF Processing\")\nprint(valid_df.info())\n\n","metadata":{"execution":{"iopub.status.busy":"2024-09-19T17:05:43.962010Z","iopub.execute_input":"2024-09-19T17:05:43.962860Z","iopub.status.idle":"2024-09-19T17:05:53.355460Z","shell.execute_reply.started":"2024-09-19T17:05:43.962813Z","shell.execute_reply":"2024-09-19T17:05:53.354541Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"Processing Text: 100%|██████████| 165767/165767 [00:04<00:00, 33807.60it/s]\nProcessing Text: 100%|██████████| 1679/1679 [00:00<00:00, 33144.46it/s]","output_type":"stream"},{"name":"stdout","text":"Trainging DF Processing\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 165767 entries, 0 to 165766\nData columns (total 4 columns):\n #   Column     Non-Null Count   Dtype \n---  ------     --------------   ----- \n 0   id         165767 non-null  object\n 1   prompt_id  165767 non-null  int64 \n 2   text       165767 non-null  object\n 3   generated  165767 non-null  int64 \ndtypes: int64(2), object(2)\nmemory usage: 5.1+ MB\nNone\nTesting DF Processing\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1679 entries, 0 to 1678\nData columns (total 4 columns):\n #   Column     Non-Null Count  Dtype \n---  ------     --------------  ----- \n 0   id         1679 non-null   object\n 1   prompt_id  1679 non-null   int64 \n 2   text       1679 non-null   object\n 3   generated  1679 non-null   int64 \ndtypes: int64(2), object(2)\nmemory usage: 52.6+ KB\nNone\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# <b><span style='color:#F1A424'>|</span> Dataset</b><a class='anchor' id='dataset'></a> [↑](#top) \n\n***\n\n    \nWe need to get the `max_len` from our `tokenizer`. We create a `tqdm` iterator and for each text we extract the tokenized length. Then we get the maximum value and we add 3 for the special tokens `CLS`, `SEP`, `SEP`.\n\n- [Hugging Face Padding and Truncation](https://huggingface.co/docs/transformers/pad_truncation): check truncation to `max_length` or `True` (batch max length).","metadata":{"papermill":{"duration":0.008127,"end_time":"2022-08-31T07:03:11.985369","exception":false,"start_time":"2022-08-31T07:03:11.977242","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"One sample from the dataset should look as following:\n```python\n{\n\t'inputs': {\n\t\t'input_ids': tensor([1, 279, 883, ..., 0, 0]),\n\t\t'token_type_ids': tensor([0, 0, 0, ..., 0, 0]),\n\t\t'attention_mask': tensor([1, 1, 1, ..., 0, 0])\n\t},\n\t'label': tensor([0.0]),\n\t'ids': '000e8c3c7ddb'\n}\n```\nYou can check it by running the cell below.","metadata":{}},{"cell_type":"markdown","source":"import wandb\n# Định nghĩa tên project để log thông tin quá trình huấn luyện trên wandb\nos.environ[\"WANDB_PROJECT\"] = \"mamba_LLM_detect_binary_classification\"\nos.environ[\"WANDB_API_KEY \"] = \"e7432690ce6d9bfdee410567f89d7e38844ed584\"\n\n\nwandb.login()\n# start a new wandb run to track this script\nwandb.init(\n    # set the wandb project where this run will be logged\n    project=\"mamba_LLM_detect_binary_classification\",\n\n    # track hyperparameters and run metadata\n    config={\n    \"learning_rate\": 6e-5,\n    \"architecture\": \"Mamba-130m-with-Linear-Head\",\n    \"dataset\": \"Test\",\n    \"epochs\": 1,\n    \"lr_scheduler_type\": \"cosine\"\n    }\n)","metadata":{}},{"cell_type":"markdown","source":"# <b><span style='color:#F1A424'>|</span> Model</b><a class='anchor' id='model'></a> [↑](#top) \n\n***","metadata":{"papermill":{"duration":0.008073,"end_time":"2022-08-31T07:03:17.933189","exception":false,"start_time":"2022-08-31T07:03:17.925116","status":"completed"},"tags":[]}},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-09-19T17:05:53.356739Z","iopub.execute_input":"2024-09-19T17:05:53.357180Z","iopub.status.idle":"2024-09-19T17:05:53.373217Z","shell.execute_reply.started":"2024-09-19T17:05:53.357130Z","shell.execute_reply":"2024-09-19T17:05:53.372238Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"           id  prompt_id                                               text  generated\n0  e_ddxvqx5i          0  In recent years, there has been a growing move...          1\n1  e_hi0yzrcv          0  \\nWhy not cars in our life\\n\\nI have ever met ...          1\n2  e_uesv4xha          0  A car is considered by many a nessecity for ev...          1\n3  e_2tl5ylwy          0  H\\n\\nello fellow citezens , we are here to inf...          0\n4  e_s6ci4vj0          0  Have you ever known how if feels not being abl...          1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>prompt_id</th>\n      <th>text</th>\n      <th>generated</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>e_ddxvqx5i</td>\n      <td>0</td>\n      <td>In recent years, there has been a growing move...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>e_hi0yzrcv</td>\n      <td>0</td>\n      <td>\\nWhy not cars in our life\\n\\nI have ever met ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>e_uesv4xha</td>\n      <td>0</td>\n      <td>A car is considered by many a nessecity for ev...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>e_2tl5ylwy</td>\n      <td>0</td>\n      <td>H\\n\\nello fellow citezens , we are here to inf...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>e_s6ci4vj0</td>\n      <td>0</td>\n      <td>Have you ever known how if feels not being abl...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\nfrom datasets import Dataset, DatasetDict\nfrom sklearn.model_selection import train_test_split\n\n# Assuming train_df is your DataFrame with a 'text' column\n# Convert the 'id' column to a string to avoid ArrowTypeError\n# df['id'] = df['id'].astype(str)\n\n# Rename the 'generated' column to 'labels'\ntrain_df.rename(columns={'generated': 'labels'}, inplace=True)\nvalid_df.rename(columns={'generated': 'labels'}, inplace=True)\n\n# # Access the train and test datasets\n# train_dataset, test_dataset = train_test_split(df, test_size=0.05)\n\n# Combine the splits into a DatasetDict\ndataset_dict = DatasetDict({\n    'train': Dataset.from_pandas(train_df),\n    'test': Dataset.from_pandas(valid_df),\n})\n\n# Display the first example from each dataset\ndataset_dict","metadata":{"execution":{"iopub.status.busy":"2024-09-19T17:05:53.374259Z","iopub.execute_input":"2024-09-19T17:05:53.374547Z","iopub.status.idle":"2024-09-19T17:05:56.661369Z","shell.execute_reply.started":"2024-09-19T17:05:53.374516Z","shell.execute_reply":"2024-09-19T17:05:56.660369Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'prompt_id', 'text', 'labels'],\n        num_rows: 165767\n    })\n    test: Dataset({\n        features: ['id', 'prompt_id', 'text', 'labels'],\n        num_rows: 1679\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neox-20b')\n# Add eos tokens\n# tokenizer.eos_token = \"<|endoftext|>\"\ntokenizer.pad_token = tokenizer.eos_token\ndef preprocess_function(examples):\n    # Tokenize the text with truncation\n    samples = tokenizer(examples['text'], \n                        truncation=True, \n                        padding='max_length', \n                        max_length=512,         \n                        return_tensors=\"pt\")\n    \n    return samples\n\n# Apply preprocessing to the dataset\ntokenized_dataset = dataset_dict.map(preprocess_function, batched=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-19T17:05:56.662437Z","iopub.execute_input":"2024-09-19T17:05:56.662755Z","iopub.status.idle":"2024-09-19T17:08:50.995345Z","shell.execute_reply.started":"2024-09-19T17:05:56.662708Z","shell.execute_reply":"2024-09-19T17:08:50.994344Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/156 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"702ba4ac593b47f5b1081e126a1d7997"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.08M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97f8d49b56d6455b8210130a177fba60"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/457k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6cc4944df1224bada155a438f1879e5f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51561ef54402453b92de58477fc2b9fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/90.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2661985793724a00a17da62718aee8ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/165767 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c263b5a72014f5496a8125c4eeef4a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1679 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be0e08a72e534a50a5f3639f5e00872c"}},"metadata":{}}]},{"cell_type":"code","source":"# Set seed cho hàm random\nrandom.seed(42)\n\n# Tạo tập train và test\ntrain_dataset = tokenized_dataset[\"train\"]\ntest_dataset = tokenized_dataset[\"test\"]\n#  Drop the 'prompt_id' feature from both datasets\ntrain_dataset = train_dataset.remove_columns([\"text\"]).remove_columns([\"id\"])\ntest_dataset = test_dataset.remove_columns([\"text\"]).remove_columns([\"id\"])\n\n# Tạo tập evaluation để đánh giá trong lúc train\n# Do số lượng tập test lớn nên chỉ lấy mẫu 1% tập dữ liệu test để đánh giá\n# total_samples = len(test_dataset)\n# eval_samples = int(0.5 * total_samples)\n# eval_indices = random.sample(range(total_samples), eval_samples)\n# eval_dataset = test_dataset.select(eval_indices)","metadata":{"execution":{"iopub.status.busy":"2024-09-19T17:08:50.996548Z","iopub.execute_input":"2024-09-19T17:08:50.996892Z","iopub.status.idle":"2024-09-19T17:08:51.011804Z","shell.execute_reply.started":"2024-09-19T17:08:50.996858Z","shell.execute_reply":"2024-09-19T17:08:51.010782Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom transformers import Trainer, TrainingArguments, DataCollatorWithPadding\n\n# Dataset and Tokenizer Setup\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n# Load the model\nFOUNDATION_MODEL_NAME = \"state-spaces/mamba-790m\"\nmodel = MambaLMHeadModel.from_pretrained(FOUNDATION_MODEL_NAME)\nmodel.lm_head = torch.nn.Linear(model.config.d_model, 2)\nmodel = nn.DataParallel(model)\nmodel.to(\"cuda\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-19T17:08:51.013088Z","iopub.execute_input":"2024-09-19T17:08:51.013411Z","iopub.status.idle":"2024-09-19T17:10:28.712704Z","shell.execute_reply.started":"2024-09-19T17:08:51.013380Z","shell.execute_reply":"2024-09-19T17:10:28.711832Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4bc97ca18b7847e6bfba7a0b5dfaf7d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/3.17G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40a524bbe83f4078be3668172a3925ac"}},"metadata":{}},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"DataParallel(\n  (module): MambaLMHeadModel(\n    (backbone): MixerModel(\n      (embedding): Embedding(50280, 1536)\n      (layers): ModuleList(\n        (0-47): 48 x Block(\n          (norm): RMSNorm()\n          (mixer): Mamba(\n            (in_proj): Linear(in_features=1536, out_features=6144, bias=False)\n            (conv1d): Conv1d(3072, 3072, kernel_size=(4,), stride=(1,), padding=(3,), groups=3072)\n            (act): SiLU()\n            (x_proj): Linear(in_features=3072, out_features=128, bias=False)\n            (dt_proj): Linear(in_features=96, out_features=3072, bias=True)\n            (out_proj): Linear(in_features=3072, out_features=1536, bias=False)\n          )\n        )\n      )\n      (norm_f): RMSNorm()\n    )\n    (lm_head): Linear(in_features=1536, out_features=2, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom tqdm import tqdm\nfrom sklearn.metrics import roc_auc_score\n\ndef TestModel(test_data_loader, model, criterion):\n    test_losses = []\n    all_predictions = []\n    all_actual_values = []\n    \n    with torch.no_grad():\n        for batch in tqdm(test_data_loader):\n            if len(batch.input_ids) == 0:\n                # Safeguard against empty sequences.\n                continue\n\n            # Have shape (batch size, token count)\n            token_sequences = batch.input_ids.cuda()\n            attention_masks = batch.attention_mask.cuda()\n            # Has shape (batch size)\n            labels = batch.labels.cuda()\n\n            with torch.cuda.amp.autocast():\n                output = model(token_sequences, attention_masks)\n\n                logits = output.logits\n                last_token_indices = torch.clamp(attention_masks.sum(dim=1) - 1, min=0)\n                raw_predictions = torch.gather(\n                    logits, \n                    dim=1, \n                    index=last_token_indices.unsqueeze(1).unsqueeze(2).expand(-1, -1, logits.shape[2])\n                ).squeeze(1)\n                \n                loss = criterion(raw_predictions, labels)\n\n            test_losses.append(loss.detach().cpu())\n\n            scaled_predictions = raw_predictions.softmax(dim=1)[:, 1]\n            all_predictions.extend(scaled_predictions.cpu().numpy())\n            all_actual_values.extend(labels.cpu().numpy())\n\n    all_predictions, all_actual_values = np.array(all_predictions), np.array(all_actual_values)\n\n    auroc = roc_auc_score(all_actual_values, all_predictions)\n\n    return auroc, np.mean(test_losses)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-19T17:10:28.715683Z","iopub.execute_input":"2024-09-19T17:10:28.716075Z","iopub.status.idle":"2024-09-19T17:10:28.726787Z","shell.execute_reply.started":"2024-09-19T17:10:28.716039Z","shell.execute_reply":"2024-09-19T17:10:28.726000Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nfrom transformers import Adafactor\n\n# Accuracy Calculation\ndef compute_accuracy(predictions, labels):\n    preds = torch.argmax(predictions, dim=1)\n    correct = torch.sum(preds == labels)\n    return correct.item() / len(labels)\n\n# Variables for the experiment\nlabel_smoothing = 0.03\noutput_subdir = '3090_1'\nmax_learning_rates = [5e-6]\n\n# Run experiment\nfor max_learning_rate in max_learning_rates:\n    print(f'lr = {max_learning_rate}, label_smoothing = {label_smoothing}, output_subdir = {output_subdir}')\n    \n    # Dataloader Setup\n    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n    train_data_loader = DataLoader(\n        train_dataset, \n        batch_size=8,  # Increased batch size since it will be split across GPUs\n        num_workers=4, \n        shuffle=True, \n        pin_memory=True, \n        collate_fn=data_collator\n    )\n    test_data_loader = DataLoader(\n        test_dataset, \n        batch_size=8,  # Increased batch size\n        num_workers=4, \n        shuffle=False, \n        pin_memory=True, \n        collate_fn=data_collator\n    )\n\n    # Optimizer, Criterion, and Scaler Setup\n    optimizer = Adafactor(\n        model.parameters(),\n        lr=max_learning_rate,\n        scale_parameter=True,\n        relative_step=False  # Fixed learning rate\n    )\n    criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n    scaler = torch.cuda.amp.GradScaler(enabled=True)\n\n    total_step_count = len(train_data_loader)\n    lr_schedule = torch.optim.lr_scheduler.OneCycleLR(\n        optimizer=optimizer,\n        max_lr=max_learning_rate,\n        total_steps=total_step_count,\n        pct_start=0.1,\n        anneal_strategy='linear',\n        cycle_momentum=False\n    )\n\n    best_auroc = -99999999\n    train_losses = []\n    model.train()\n\n    # Tracking the number of rows processed\n    total_rows_processed = 0\n    row_threshold = 50000\n\n    print_steps = 500  # Log training accuracy/loss every 500 steps\n\n    for batch_index, train_batch in enumerate(tqdm(train_data_loader)):\n        if len(train_batch.input_ids) == 0:\n            continue\n\n        # Send data to GPU(s)\n        token_sequences = train_batch.input_ids.to(\"cuda\")\n        attention_masks = train_batch.attention_mask.to(\"cuda\")\n        labels = train_batch.labels.to(\"cuda\")\n\n        optimizer.zero_grad()\n\n        with torch.cuda.amp.autocast(enabled=True):\n            output = model(token_sequences, attention_masks)\n            logits = output.logits\n            last_token_indices = torch.clamp(attention_masks.sum(dim=1) - 1, min=0)\n            raw_predictions = torch.gather(\n                logits, dim=1, index=last_token_indices.unsqueeze(1).unsqueeze(2).expand(-1, -1, logits.shape[2])\n            ).squeeze(1)\n\n            loss = criterion(raw_predictions, labels)\n\n        # Training accuracy\n        accuracy = compute_accuracy(raw_predictions, labels)\n\n        scaler.scale(loss).backward()\n        scaler.unscale_(optimizer)\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        scaler.step(optimizer)\n        scaler.update()\n        lr_schedule.step()\n\n        train_losses.append(loss.detach().cpu())\n\n        # Log training accuracy and loss every 500 steps\n        if (batch_index + 1) % print_steps == 0:\n            avg_train_loss = sum(train_losses) / len(train_losses)\n            print(f\"Step {batch_index+1}/{total_step_count}: Avg Train Loss = {avg_train_loss:.4f}, Train Accuracy = {accuracy*100:.2f}%\")\n            train_losses = []  # Reset train loss tracking for the next 500 steps\n\n        # Increment the number of rows processed\n        total_rows_processed += len(train_batch.input_ids)\n\n        # Evaluate the model every 50,000 rows\n        if total_rows_processed >= row_threshold:\n            model.eval()\n            val_accuracy, test_loss = TestModel(test_data_loader, model, criterion)\n            model.train()\n            \n            print(f'Validation Loss: {test_loss:.4f}, Validation Accuracy: {val_accuracy*100:.2f}%')\n            \n            total_rows_processed = 0  # Reset after each evaluation\n\n    # Save model and reset\n    torch.save(model.state_dict(), f'/kaggle/working/Models/Mamba-780m-Step-{batch_index+1}-Loss-{int(test_loss*1000)}.pth')\n    print(f'Best AUROC: {best_auroc}')\n","metadata":{"execution":{"iopub.status.busy":"2024-09-19T17:10:28.728385Z","iopub.execute_input":"2024-09-19T17:10:28.728787Z","iopub.status.idle":"2024-09-20T03:37:05.179864Z","shell.execute_reply.started":"2024-09-19T17:10:28.728728Z","shell.execute_reply":"2024-09-20T03:37:05.178362Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"lr = 5e-06, label_smoothing = 0.03, output_subdir = 3090_1\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/20721 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n  2%|▏         | 500/20721 [15:20<10:09:15,  1.81s/it]","output_type":"stream"},{"name":"stdout","text":"Step 500/20721: Avg Train Loss = 1.0302, Train Accuracy = 62.50%\n","output_type":"stream"},{"name":"stderr","text":"  5%|▍         | 1000/20721 [30:20<9:50:06,  1.80s/it]","output_type":"stream"},{"name":"stdout","text":"Step 1000/20721: Avg Train Loss = 0.8935, Train Accuracy = 50.00%\n","output_type":"stream"},{"name":"stderr","text":"  7%|▋         | 1500/20721 [45:19<9:36:18,  1.80s/it]","output_type":"stream"},{"name":"stdout","text":"Step 1500/20721: Avg Train Loss = 0.7625, Train Accuracy = 87.50%\n","output_type":"stream"},{"name":"stderr","text":" 10%|▉         | 2000/20721 [1:00:18<9:21:24,  1.80s/it]","output_type":"stream"},{"name":"stdout","text":"Step 2000/20721: Avg Train Loss = 0.6464, Train Accuracy = 75.00%\n","output_type":"stream"},{"name":"stderr","text":" 12%|█▏        | 2500/20721 [1:15:16<9:05:11,  1.80s/it]","output_type":"stream"},{"name":"stdout","text":"Step 2500/20721: Avg Train Loss = 0.6076, Train Accuracy = 75.00%\n","output_type":"stream"},{"name":"stderr","text":" 14%|█▍        | 3000/20721 [1:30:16<8:50:21,  1.80s/it]","output_type":"stream"},{"name":"stdout","text":"Step 3000/20721: Avg Train Loss = 0.5415, Train Accuracy = 87.50%\n","output_type":"stream"},{"name":"stderr","text":" 17%|█▋        | 3500/20721 [1:45:15<8:35:26,  1.80s/it]","output_type":"stream"},{"name":"stdout","text":"Step 3500/20721: Avg Train Loss = 0.5221, Train Accuracy = 75.00%\n","output_type":"stream"},{"name":"stderr","text":" 19%|█▉        | 4000/20721 [2:00:14<8:21:01,  1.80s/it]","output_type":"stream"},{"name":"stdout","text":"Step 4000/20721: Avg Train Loss = 0.5045, Train Accuracy = 87.50%\n","output_type":"stream"},{"name":"stderr","text":" 22%|██▏       | 4500/20721 [2:15:13<8:05:43,  1.80s/it]","output_type":"stream"},{"name":"stdout","text":"Step 4500/20721: Avg Train Loss = 0.4931, Train Accuracy = 75.00%\n","output_type":"stream"},{"name":"stderr","text":" 24%|██▍       | 5000/20721 [2:30:11<7:51:03,  1.80s/it]","output_type":"stream"},{"name":"stdout","text":"Step 5000/20721: Avg Train Loss = 0.4797, Train Accuracy = 87.50%\n","output_type":"stream"},{"name":"stderr","text":" 27%|██▋       | 5500/20721 [2:45:10<7:35:45,  1.80s/it]","output_type":"stream"},{"name":"stdout","text":"Step 5500/20721: Avg Train Loss = 0.4705, Train Accuracy = 75.00%\n","output_type":"stream"},{"name":"stderr","text":" 29%|██▉       | 6000/20721 [3:00:08<7:22:06,  1.80s/it]","output_type":"stream"},{"name":"stdout","text":"Step 6000/20721: Avg Train Loss = 0.4773, Train Accuracy = 75.00%\n","output_type":"stream"},{"name":"stderr","text":" 30%|███       | 6249/20721 [3:07:36<7:12:58,  1.80s/it]\n  0%|          | 0/210 [00:00<?, ?it/s]\u001b[Ahuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n\n  0%|          | 1/210 [00:00<02:57,  1.18it/s]\u001b[A\n  1%|          | 2/210 [00:01<02:19,  1.49it/s]\u001b[A\n  1%|▏         | 3/210 [00:01<02:06,  1.64it/s]\u001b[A\n  2%|▏         | 4/210 [00:02<01:59,  1.72it/s]\u001b[A\n  2%|▏         | 5/210 [00:03<01:55,  1.77it/s]\u001b[A\n  3%|▎         | 6/210 [00:03<01:53,  1.80it/s]\u001b[A\n  3%|▎         | 7/210 [00:04<01:51,  1.82it/s]\u001b[A\n  4%|▍         | 8/210 [00:04<01:50,  1.83it/s]\u001b[A\n  4%|▍         | 9/210 [00:05<01:49,  1.84it/s]\u001b[A\n  5%|▍         | 10/210 [00:05<01:48,  1.85it/s]\u001b[A\n  5%|▌         | 11/210 [00:06<01:47,  1.85it/s]\u001b[A\n  6%|▌         | 12/210 [00:06<01:47,  1.85it/s]\u001b[A\n  6%|▌         | 13/210 [00:07<01:58,  1.67it/s]\u001b[A\n  7%|▋         | 14/210 [00:08<01:53,  1.73it/s]\u001b[A\n  7%|▋         | 15/210 [00:08<01:50,  1.76it/s]\u001b[A\n  8%|▊         | 16/210 [00:09<01:48,  1.79it/s]\u001b[A\n  8%|▊         | 17/210 [00:09<01:46,  1.80it/s]\u001b[A\n  9%|▊         | 18/210 [00:10<01:45,  1.82it/s]\u001b[A\n  9%|▉         | 19/210 [00:10<01:44,  1.83it/s]\u001b[A\n 10%|▉         | 20/210 [00:11<01:43,  1.84it/s]\u001b[A\n 10%|█         | 21/210 [00:11<01:42,  1.85it/s]\u001b[A\n 10%|█         | 22/210 [00:12<01:41,  1.85it/s]\u001b[A\n 11%|█         | 23/210 [00:12<01:41,  1.85it/s]\u001b[A\n 11%|█▏        | 24/210 [00:13<01:40,  1.85it/s]\u001b[A\n 12%|█▏        | 25/210 [00:13<01:40,  1.85it/s]\u001b[A\n 12%|█▏        | 26/210 [00:14<01:39,  1.85it/s]\u001b[A\n 13%|█▎        | 27/210 [00:15<01:38,  1.85it/s]\u001b[A\n 13%|█▎        | 28/210 [00:15<01:38,  1.86it/s]\u001b[A\n 14%|█▍        | 29/210 [00:16<01:37,  1.85it/s]\u001b[A\n 14%|█▍        | 30/210 [00:16<01:37,  1.85it/s]\u001b[A\n 15%|█▍        | 31/210 [00:17<01:36,  1.85it/s]\u001b[A\n 15%|█▌        | 32/210 [00:17<01:36,  1.85it/s]\u001b[A\n 16%|█▌        | 33/210 [00:18<01:35,  1.85it/s]\u001b[A\n 16%|█▌        | 34/210 [00:18<01:34,  1.85it/s]\u001b[A\n 17%|█▋        | 35/210 [00:19<01:34,  1.86it/s]\u001b[A\n 17%|█▋        | 36/210 [00:19<01:33,  1.86it/s]\u001b[A\n 18%|█▊        | 37/210 [00:20<01:33,  1.86it/s]\u001b[A\n 18%|█▊        | 38/210 [00:20<01:32,  1.86it/s]\u001b[A\n 19%|█▊        | 39/210 [00:21<01:32,  1.86it/s]\u001b[A\n 19%|█▉        | 40/210 [00:22<01:31,  1.85it/s]\u001b[A\n 20%|█▉        | 41/210 [00:22<01:31,  1.85it/s]\u001b[A\n 20%|██        | 42/210 [00:23<01:30,  1.85it/s]\u001b[A\n 20%|██        | 43/210 [00:23<01:30,  1.85it/s]\u001b[A\n 21%|██        | 44/210 [00:24<01:29,  1.85it/s]\u001b[A\n 21%|██▏       | 45/210 [00:24<01:29,  1.85it/s]\u001b[A\n 22%|██▏       | 46/210 [00:25<01:28,  1.85it/s]\u001b[A\n 22%|██▏       | 47/210 [00:25<01:28,  1.85it/s]\u001b[A\n 23%|██▎       | 48/210 [00:26<01:27,  1.85it/s]\u001b[A\n 23%|██▎       | 49/210 [00:26<01:26,  1.86it/s]\u001b[A\n 24%|██▍       | 50/210 [00:27<01:26,  1.86it/s]\u001b[A\n 24%|██▍       | 51/210 [00:28<01:25,  1.85it/s]\u001b[A\n 25%|██▍       | 52/210 [00:28<01:25,  1.85it/s]\u001b[A\n 25%|██▌       | 53/210 [00:29<01:24,  1.85it/s]\u001b[A\n 26%|██▌       | 54/210 [00:29<01:23,  1.86it/s]\u001b[A\n 26%|██▌       | 55/210 [00:30<01:23,  1.86it/s]\u001b[A\n 27%|██▋       | 56/210 [00:30<01:23,  1.85it/s]\u001b[A\n 27%|██▋       | 57/210 [00:31<01:22,  1.85it/s]\u001b[A\n 28%|██▊       | 58/210 [00:31<01:22,  1.85it/s]\u001b[A\n 28%|██▊       | 59/210 [00:32<01:21,  1.85it/s]\u001b[A\n 29%|██▊       | 60/210 [00:32<01:20,  1.85it/s]\u001b[A\n 29%|██▉       | 61/210 [00:33<01:20,  1.86it/s]\u001b[A\n 30%|██▉       | 62/210 [00:33<01:19,  1.85it/s]\u001b[A\n 30%|███       | 63/210 [00:34<01:19,  1.86it/s]\u001b[A\n 30%|███       | 64/210 [00:35<01:18,  1.85it/s]\u001b[A\n 31%|███       | 65/210 [00:35<01:18,  1.85it/s]\u001b[A\n 31%|███▏      | 66/210 [00:36<01:17,  1.86it/s]\u001b[A\n 32%|███▏      | 67/210 [00:36<01:17,  1.85it/s]\u001b[A\n 32%|███▏      | 68/210 [00:37<01:16,  1.85it/s]\u001b[A\n 33%|███▎      | 69/210 [00:37<01:15,  1.86it/s]\u001b[A\n 33%|███▎      | 70/210 [00:38<01:15,  1.86it/s]\u001b[A\n 34%|███▍      | 71/210 [00:38<01:14,  1.86it/s]\u001b[A\n 34%|███▍      | 72/210 [00:39<01:14,  1.85it/s]\u001b[A\n 35%|███▍      | 73/210 [00:39<01:13,  1.86it/s]\u001b[A\n 35%|███▌      | 74/210 [00:40<01:13,  1.86it/s]\u001b[A\n 36%|███▌      | 75/210 [00:40<01:12,  1.85it/s]\u001b[A\n 36%|███▌      | 76/210 [00:41<01:12,  1.85it/s]\u001b[A\n 37%|███▋      | 77/210 [00:42<01:11,  1.86it/s]\u001b[A\n 37%|███▋      | 78/210 [00:42<01:11,  1.86it/s]\u001b[A\n 38%|███▊      | 79/210 [00:43<01:10,  1.85it/s]\u001b[A\n 38%|███▊      | 80/210 [00:43<01:10,  1.85it/s]\u001b[A\n 39%|███▊      | 81/210 [00:44<01:09,  1.86it/s]\u001b[A\n 39%|███▉      | 82/210 [00:44<01:10,  1.81it/s]\u001b[A\n 40%|███▉      | 83/210 [00:45<01:09,  1.82it/s]\u001b[A\n 40%|████      | 84/210 [00:45<01:08,  1.83it/s]\u001b[A\n 40%|████      | 85/210 [00:46<01:08,  1.84it/s]\u001b[A\n 41%|████      | 86/210 [00:46<01:07,  1.84it/s]\u001b[A\n 41%|████▏     | 87/210 [00:47<01:06,  1.85it/s]\u001b[A\n 42%|████▏     | 88/210 [00:48<01:06,  1.84it/s]\u001b[A\n 42%|████▏     | 89/210 [00:48<01:05,  1.85it/s]\u001b[A\n 43%|████▎     | 90/210 [00:49<01:04,  1.85it/s]\u001b[A\n 43%|████▎     | 91/210 [00:49<01:04,  1.85it/s]\u001b[A\n 44%|████▍     | 92/210 [00:50<01:03,  1.85it/s]\u001b[A\n 44%|████▍     | 93/210 [00:50<01:03,  1.85it/s]\u001b[A\n 45%|████▍     | 94/210 [00:51<01:02,  1.85it/s]\u001b[A\n 45%|████▌     | 95/210 [00:51<01:02,  1.85it/s]\u001b[A\n 46%|████▌     | 96/210 [00:52<01:01,  1.85it/s]\u001b[A\n 46%|████▌     | 97/210 [00:52<01:01,  1.85it/s]\u001b[A\n 47%|████▋     | 98/210 [00:53<01:00,  1.85it/s]\u001b[A\n 47%|████▋     | 99/210 [00:53<00:59,  1.85it/s]\u001b[A\n 48%|████▊     | 100/210 [00:54<00:59,  1.85it/s]\u001b[A\n 48%|████▊     | 101/210 [00:55<00:58,  1.85it/s]\u001b[A\n 49%|████▊     | 102/210 [00:55<00:58,  1.85it/s]\u001b[A\n 49%|████▉     | 103/210 [00:56<00:57,  1.85it/s]\u001b[A\n 50%|████▉     | 104/210 [00:56<00:57,  1.86it/s]\u001b[A\n 50%|█████     | 105/210 [00:57<00:56,  1.85it/s]\u001b[A\n 50%|█████     | 106/210 [00:57<00:56,  1.85it/s]\u001b[A\n 51%|█████     | 107/210 [00:58<00:55,  1.85it/s]\u001b[A\n 51%|█████▏    | 108/210 [00:58<00:55,  1.83it/s]\u001b[A\n 52%|█████▏    | 109/210 [00:59<00:54,  1.84it/s]\u001b[A\n 52%|█████▏    | 110/210 [00:59<00:54,  1.84it/s]\u001b[A\n 53%|█████▎    | 111/210 [01:00<00:53,  1.84it/s]\u001b[A\n 53%|█████▎    | 112/210 [01:00<00:53,  1.84it/s]\u001b[A\n 54%|█████▍    | 113/210 [01:01<00:52,  1.85it/s]\u001b[A\n 54%|█████▍    | 114/210 [01:02<00:51,  1.85it/s]\u001b[A\n 55%|█████▍    | 115/210 [01:02<00:51,  1.84it/s]\u001b[A\n 55%|█████▌    | 116/210 [01:03<00:51,  1.84it/s]\u001b[A\n 56%|█████▌    | 117/210 [01:03<00:50,  1.84it/s]\u001b[A\n 56%|█████▌    | 118/210 [01:04<00:49,  1.84it/s]\u001b[A\n 57%|█████▋    | 119/210 [01:04<00:49,  1.84it/s]\u001b[A\n 57%|█████▋    | 120/210 [01:05<00:48,  1.85it/s]\u001b[A\n 58%|█████▊    | 121/210 [01:05<00:47,  1.85it/s]\u001b[A\n 58%|█████▊    | 122/210 [01:06<00:47,  1.85it/s]\u001b[A\n 59%|█████▊    | 123/210 [01:06<00:46,  1.85it/s]\u001b[A\n 59%|█████▉    | 124/210 [01:07<00:46,  1.86it/s]\u001b[A\n 60%|█████▉    | 125/210 [01:08<00:45,  1.85it/s]\u001b[A\n 60%|██████    | 126/210 [01:08<00:45,  1.85it/s]\u001b[A\n 60%|██████    | 127/210 [01:09<00:44,  1.86it/s]\u001b[A\n 61%|██████    | 128/210 [01:09<00:44,  1.86it/s]\u001b[A\n 61%|██████▏   | 129/210 [01:10<00:43,  1.86it/s]\u001b[A\n 62%|██████▏   | 130/210 [01:10<00:43,  1.85it/s]\u001b[A\n 62%|██████▏   | 131/210 [01:11<00:42,  1.85it/s]\u001b[A\n 63%|██████▎   | 132/210 [01:11<00:42,  1.86it/s]\u001b[A\n 63%|██████▎   | 133/210 [01:12<00:41,  1.85it/s]\u001b[A\n 64%|██████▍   | 134/210 [01:12<00:41,  1.85it/s]\u001b[A\n 64%|██████▍   | 135/210 [01:13<00:40,  1.85it/s]\u001b[A\n 65%|██████▍   | 136/210 [01:13<00:40,  1.84it/s]\u001b[A\n 65%|██████▌   | 137/210 [01:14<00:39,  1.85it/s]\u001b[A\n 66%|██████▌   | 138/210 [01:15<00:38,  1.85it/s]\u001b[A\n 66%|██████▌   | 139/210 [01:15<00:38,  1.85it/s]\u001b[A\n 67%|██████▋   | 140/210 [01:16<00:37,  1.86it/s]\u001b[A\n 67%|██████▋   | 141/210 [01:16<00:37,  1.86it/s]\u001b[A\n 68%|██████▊   | 142/210 [01:17<00:36,  1.85it/s]\u001b[A\n 68%|██████▊   | 143/210 [01:17<00:36,  1.85it/s]\u001b[A\n 69%|██████▊   | 144/210 [01:18<00:35,  1.85it/s]\u001b[A\n 69%|██████▉   | 145/210 [01:18<00:35,  1.85it/s]\u001b[A\n 70%|██████▉   | 146/210 [01:19<00:34,  1.85it/s]\u001b[A\n 70%|███████   | 147/210 [01:19<00:34,  1.85it/s]\u001b[A\n 70%|███████   | 148/210 [01:20<00:33,  1.85it/s]\u001b[A\n 71%|███████   | 149/210 [01:20<00:32,  1.86it/s]\u001b[A\n 71%|███████▏  | 150/210 [01:21<00:32,  1.86it/s]\u001b[A\n 72%|███████▏  | 151/210 [01:22<00:32,  1.82it/s]\u001b[A\n 72%|███████▏  | 152/210 [01:22<00:31,  1.83it/s]\u001b[A\n 73%|███████▎  | 153/210 [01:23<00:31,  1.83it/s]\u001b[A\n 73%|███████▎  | 154/210 [01:23<00:30,  1.84it/s]\u001b[A\n 74%|███████▍  | 155/210 [01:24<00:29,  1.85it/s]\u001b[A\n 74%|███████▍  | 156/210 [01:24<00:29,  1.85it/s]\u001b[A\n 75%|███████▍  | 157/210 [01:25<00:28,  1.86it/s]\u001b[A\n 75%|███████▌  | 158/210 [01:25<00:27,  1.86it/s]\u001b[A\n 76%|███████▌  | 159/210 [01:26<00:27,  1.87it/s]\u001b[A\n 76%|███████▌  | 160/210 [01:26<00:26,  1.87it/s]\u001b[A\n 77%|███████▋  | 161/210 [01:27<00:26,  1.87it/s]\u001b[A\n 77%|███████▋  | 162/210 [01:27<00:25,  1.87it/s]\u001b[A\n 78%|███████▊  | 163/210 [01:28<00:25,  1.86it/s]\u001b[A\n 78%|███████▊  | 164/210 [01:29<00:24,  1.86it/s]\u001b[A\n 79%|███████▊  | 165/210 [01:29<00:24,  1.87it/s]\u001b[A\n 79%|███████▉  | 166/210 [01:30<00:23,  1.87it/s]\u001b[A\n 80%|███████▉  | 167/210 [01:30<00:23,  1.86it/s]\u001b[A\n 80%|████████  | 168/210 [01:31<00:22,  1.86it/s]\u001b[A\n 80%|████████  | 169/210 [01:31<00:22,  1.86it/s]\u001b[A\n 81%|████████  | 170/210 [01:32<00:21,  1.86it/s]\u001b[A\n 81%|████████▏ | 171/210 [01:32<00:21,  1.86it/s]\u001b[A\n 82%|████████▏ | 172/210 [01:33<00:20,  1.86it/s]\u001b[A\n 82%|████████▏ | 173/210 [01:33<00:19,  1.86it/s]\u001b[A\n 83%|████████▎ | 174/210 [01:34<00:19,  1.86it/s]\u001b[A\n 83%|████████▎ | 175/210 [01:34<00:18,  1.86it/s]\u001b[A\n 84%|████████▍ | 176/210 [01:35<00:18,  1.86it/s]\u001b[A\n 84%|████████▍ | 177/210 [01:36<00:17,  1.85it/s]\u001b[A\n 85%|████████▍ | 178/210 [01:36<00:17,  1.86it/s]\u001b[A\n 85%|████████▌ | 179/210 [01:37<00:16,  1.86it/s]\u001b[A\n 86%|████████▌ | 180/210 [01:37<00:16,  1.86it/s]\u001b[A\n 86%|████████▌ | 181/210 [01:38<00:15,  1.86it/s]\u001b[A\n 87%|████████▋ | 182/210 [01:38<00:15,  1.86it/s]\u001b[A\n 87%|████████▋ | 183/210 [01:39<00:14,  1.86it/s]\u001b[A\n 88%|████████▊ | 184/210 [01:39<00:14,  1.86it/s]\u001b[A\n 88%|████████▊ | 185/210 [01:40<00:13,  1.86it/s]\u001b[A\n 89%|████████▊ | 186/210 [01:40<00:12,  1.86it/s]\u001b[A\n 89%|████████▉ | 187/210 [01:41<00:12,  1.86it/s]\u001b[A\n 90%|████████▉ | 188/210 [01:41<00:11,  1.85it/s]\u001b[A\n 90%|█████████ | 189/210 [01:42<00:11,  1.86it/s]\u001b[A\n 90%|█████████ | 190/210 [01:43<00:10,  1.86it/s]\u001b[A\n 91%|█████████ | 191/210 [01:43<00:10,  1.85it/s]\u001b[A\n 91%|█████████▏| 192/210 [01:44<00:09,  1.86it/s]\u001b[A\n 92%|█████████▏| 193/210 [01:44<00:09,  1.85it/s]\u001b[A\n 92%|█████████▏| 194/210 [01:45<00:08,  1.85it/s]\u001b[A\n 93%|█████████▎| 195/210 [01:45<00:08,  1.86it/s]\u001b[A\n 93%|█████████▎| 196/210 [01:46<00:07,  1.86it/s]\u001b[A\n 94%|█████████▍| 197/210 [01:46<00:07,  1.86it/s]\u001b[A\n 94%|█████████▍| 198/210 [01:47<00:06,  1.85it/s]\u001b[A\n 95%|█████████▍| 199/210 [01:47<00:05,  1.85it/s]\u001b[A\n 95%|█████████▌| 200/210 [01:48<00:05,  1.85it/s]\u001b[A\n 96%|█████████▌| 201/210 [01:48<00:04,  1.85it/s]\u001b[A\n 96%|█████████▌| 202/210 [01:49<00:04,  1.85it/s]\u001b[A\n 97%|█████████▋| 203/210 [01:50<00:03,  1.85it/s]\u001b[A\n 97%|█████████▋| 204/210 [01:50<00:03,  1.85it/s]\u001b[A\n 98%|█████████▊| 205/210 [01:51<00:02,  1.86it/s]\u001b[A\n 98%|█████████▊| 206/210 [01:51<00:02,  1.86it/s]\u001b[A\n 99%|█████████▊| 207/210 [01:52<00:01,  1.86it/s]\u001b[A\n 99%|█████████▉| 208/210 [01:52<00:01,  1.86it/s]\u001b[A\n100%|█████████▉| 209/210 [01:53<00:00,  1.86it/s]\u001b[A\n100%|██████████| 210/210 [01:53<00:00,  1.84it/s]\u001b[A\n 30%|███       | 6250/20721 [3:09:32<144:38:38, 35.98s/it]","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.4388, Validation Accuracy: 80.31%\n","output_type":"stream"},{"name":"stderr","text":" 31%|███▏      | 6500/20721 [3:17:01<7:04:23,  1.79s/it]  ","output_type":"stream"},{"name":"stdout","text":"Step 6500/20721: Avg Train Loss = 0.4655, Train Accuracy = 50.00%\n","output_type":"stream"},{"name":"stderr","text":" 34%|███▍      | 7000/20721 [3:31:59<6:48:59,  1.79s/it]","output_type":"stream"},{"name":"stdout","text":"Step 7000/20721: Avg Train Loss = 0.4525, Train Accuracy = 100.00%\n","output_type":"stream"},{"name":"stderr","text":" 36%|███▌      | 7500/20721 [3:46:58<6:38:14,  1.81s/it]","output_type":"stream"},{"name":"stdout","text":"Step 7500/20721: Avg Train Loss = 0.4424, Train Accuracy = 75.00%\n","output_type":"stream"},{"name":"stderr","text":" 39%|███▊      | 8000/20721 [4:01:56<6:22:00,  1.80s/it]","output_type":"stream"},{"name":"stdout","text":"Step 8000/20721: Avg Train Loss = 0.4315, Train Accuracy = 100.00%\n","output_type":"stream"},{"name":"stderr","text":" 41%|████      | 8500/20721 [4:16:54<6:06:48,  1.80s/it]","output_type":"stream"},{"name":"stdout","text":"Step 8500/20721: Avg Train Loss = 0.4339, Train Accuracy = 87.50%\n","output_type":"stream"},{"name":"stderr","text":" 43%|████▎     | 9000/20721 [4:31:52<5:51:36,  1.80s/it]","output_type":"stream"},{"name":"stdout","text":"Step 9000/20721: Avg Train Loss = 0.4256, Train Accuracy = 100.00%\n","output_type":"stream"},{"name":"stderr","text":" 46%|████▌     | 9500/20721 [4:46:49<5:35:26,  1.79s/it]","output_type":"stream"},{"name":"stdout","text":"Step 9500/20721: Avg Train Loss = 0.4145, Train Accuracy = 100.00%\n","output_type":"stream"},{"name":"stderr","text":" 48%|████▊     | 10000/20721 [5:01:47<5:20:35,  1.79s/it]","output_type":"stream"},{"name":"stdout","text":"Step 10000/20721: Avg Train Loss = 0.4130, Train Accuracy = 62.50%\n","output_type":"stream"},{"name":"stderr","text":" 51%|█████     | 10500/20721 [5:16:45<5:05:26,  1.79s/it]","output_type":"stream"},{"name":"stdout","text":"Step 10500/20721: Avg Train Loss = 0.4186, Train Accuracy = 75.00%\n","output_type":"stream"},{"name":"stderr","text":" 53%|█████▎    | 11000/20721 [5:31:43<4:50:26,  1.79s/it]","output_type":"stream"},{"name":"stdout","text":"Step 11000/20721: Avg Train Loss = 0.3907, Train Accuracy = 75.00%\n","output_type":"stream"},{"name":"stderr","text":" 55%|█████▌    | 11500/20721 [5:46:41<4:36:08,  1.80s/it]","output_type":"stream"},{"name":"stdout","text":"Step 11500/20721: Avg Train Loss = 0.4148, Train Accuracy = 75.00%\n","output_type":"stream"},{"name":"stderr","text":" 58%|█████▊    | 12000/20721 [6:01:39<4:20:31,  1.79s/it]","output_type":"stream"},{"name":"stdout","text":"Step 12000/20721: Avg Train Loss = 0.3950, Train Accuracy = 100.00%\n","output_type":"stream"},{"name":"stderr","text":" 60%|██████    | 12499/20721 [6:16:35<4:06:36,  1.80s/it]","output_type":"stream"},{"name":"stdout","text":"Step 12500/20721: Avg Train Loss = 0.4105, Train Accuracy = 87.50%\n","output_type":"stream"},{"name":"stderr","text":"\n  0%|          | 0/210 [00:00<?, ?it/s]\u001b[Ahuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n\n  0%|          | 1/210 [00:00<02:53,  1.21it/s]\u001b[A\n  1%|          | 2/210 [00:01<02:40,  1.30it/s]\u001b[A\n  1%|▏         | 3/210 [00:02<02:17,  1.50it/s]\u001b[A\n  2%|▏         | 4/210 [00:02<02:06,  1.63it/s]\u001b[A\n  2%|▏         | 5/210 [00:03<02:00,  1.71it/s]\u001b[A\n  3%|▎         | 6/210 [00:03<01:56,  1.75it/s]\u001b[A\n  3%|▎         | 7/210 [00:04<01:53,  1.78it/s]\u001b[A\n  4%|▍         | 8/210 [00:04<01:52,  1.80it/s]\u001b[A\n  4%|▍         | 9/210 [00:05<01:50,  1.82it/s]\u001b[A\n  5%|▍         | 10/210 [00:05<01:49,  1.83it/s]\u001b[A\n  5%|▌         | 11/210 [00:06<01:48,  1.84it/s]\u001b[A\n  6%|▌         | 12/210 [00:06<01:47,  1.84it/s]\u001b[A\n  6%|▌         | 13/210 [00:07<01:46,  1.84it/s]\u001b[A\n  7%|▋         | 14/210 [00:08<01:46,  1.85it/s]\u001b[A\n  7%|▋         | 15/210 [00:08<01:45,  1.85it/s]\u001b[A\n  8%|▊         | 16/210 [00:09<01:44,  1.85it/s]\u001b[A\n  8%|▊         | 17/210 [00:09<01:44,  1.86it/s]\u001b[A\n  9%|▊         | 18/210 [00:10<01:43,  1.85it/s]\u001b[A\n  9%|▉         | 19/210 [00:10<01:42,  1.86it/s]\u001b[A\n 10%|▉         | 20/210 [00:11<01:42,  1.86it/s]\u001b[A\n 10%|█         | 21/210 [00:11<01:41,  1.86it/s]\u001b[A\n 10%|█         | 22/210 [00:12<01:41,  1.86it/s]\u001b[A\n 11%|█         | 23/210 [00:12<01:40,  1.86it/s]\u001b[A\n 11%|█▏        | 24/210 [00:13<01:39,  1.87it/s]\u001b[A\n 12%|█▏        | 25/210 [00:13<01:39,  1.86it/s]\u001b[A\n 12%|█▏        | 26/210 [00:14<01:39,  1.86it/s]\u001b[A\n 13%|█▎        | 27/210 [00:15<01:38,  1.86it/s]\u001b[A\n 13%|█▎        | 28/210 [00:15<01:37,  1.86it/s]\u001b[A\n 14%|█▍        | 29/210 [00:16<01:37,  1.86it/s]\u001b[A\n 14%|█▍        | 30/210 [00:16<01:36,  1.86it/s]\u001b[A\n 15%|█▍        | 31/210 [00:17<01:36,  1.86it/s]\u001b[A\n 15%|█▌        | 32/210 [00:17<01:35,  1.86it/s]\u001b[A\n 16%|█▌        | 33/210 [00:18<01:35,  1.86it/s]\u001b[A\n 16%|█▌        | 34/210 [00:18<01:34,  1.86it/s]\u001b[A\n 17%|█▋        | 35/210 [00:19<01:34,  1.86it/s]\u001b[A\n 17%|█▋        | 36/210 [00:19<01:33,  1.86it/s]\u001b[A\n 18%|█▊        | 37/210 [00:20<01:32,  1.86it/s]\u001b[A\n 18%|█▊        | 38/210 [00:20<01:32,  1.86it/s]\u001b[A\n 19%|█▊        | 39/210 [00:21<01:32,  1.86it/s]\u001b[A\n 19%|█▉        | 40/210 [00:22<01:31,  1.86it/s]\u001b[A\n 20%|█▉        | 41/210 [00:22<01:31,  1.86it/s]\u001b[A\n 20%|██        | 42/210 [00:23<01:30,  1.86it/s]\u001b[A\n 20%|██        | 43/210 [00:23<01:29,  1.86it/s]\u001b[A\n 21%|██        | 44/210 [00:24<01:29,  1.85it/s]\u001b[A\n 21%|██▏       | 45/210 [00:24<01:28,  1.86it/s]\u001b[A\n 22%|██▏       | 46/210 [00:25<01:28,  1.86it/s]\u001b[A\n 22%|██▏       | 47/210 [00:25<01:27,  1.86it/s]\u001b[A\n 23%|██▎       | 48/210 [00:26<01:27,  1.86it/s]\u001b[A\n 23%|██▎       | 49/210 [00:26<01:26,  1.86it/s]\u001b[A\n 24%|██▍       | 50/210 [00:27<01:26,  1.85it/s]\u001b[A\n 24%|██▍       | 51/210 [00:27<01:25,  1.85it/s]\u001b[A\n 25%|██▍       | 52/210 [00:28<01:25,  1.85it/s]\u001b[A\n 25%|██▌       | 53/210 [00:29<01:24,  1.85it/s]\u001b[A\n 26%|██▌       | 54/210 [00:29<01:24,  1.85it/s]\u001b[A\n 26%|██▌       | 55/210 [00:30<01:23,  1.85it/s]\u001b[A\n 27%|██▋       | 56/210 [00:30<01:23,  1.85it/s]\u001b[A\n 27%|██▋       | 57/210 [00:31<01:22,  1.85it/s]\u001b[A\n 28%|██▊       | 58/210 [00:31<01:21,  1.86it/s]\u001b[A\n 28%|██▊       | 59/210 [00:32<01:21,  1.86it/s]\u001b[A\n 29%|██▊       | 60/210 [00:32<01:20,  1.86it/s]\u001b[A\n 29%|██▉       | 61/210 [00:33<01:20,  1.86it/s]\u001b[A\n 30%|██▉       | 62/210 [00:33<01:19,  1.85it/s]\u001b[A\n 30%|███       | 63/210 [00:34<01:19,  1.85it/s]\u001b[A\n 30%|███       | 64/210 [00:34<01:18,  1.86it/s]\u001b[A\n 31%|███       | 65/210 [00:35<01:17,  1.86it/s]\u001b[A\n 31%|███▏      | 66/210 [00:36<01:17,  1.86it/s]\u001b[A\n 32%|███▏      | 67/210 [00:36<01:16,  1.86it/s]\u001b[A\n 32%|███▏      | 68/210 [00:37<01:16,  1.86it/s]\u001b[A\n 33%|███▎      | 69/210 [00:37<01:15,  1.87it/s]\u001b[A\n 33%|███▎      | 70/210 [00:38<01:15,  1.86it/s]\u001b[A\n 34%|███▍      | 71/210 [00:38<01:14,  1.86it/s]\u001b[A\n 34%|███▍      | 72/210 [00:39<01:14,  1.86it/s]\u001b[A\n 35%|███▍      | 73/210 [00:39<01:13,  1.86it/s]\u001b[A\n 35%|███▌      | 74/210 [00:40<01:12,  1.87it/s]\u001b[A\n 36%|███▌      | 75/210 [00:40<01:12,  1.87it/s]\u001b[A\n 36%|███▌      | 76/210 [00:41<01:11,  1.86it/s]\u001b[A\n 37%|███▋      | 77/210 [00:41<01:11,  1.87it/s]\u001b[A\n 37%|███▋      | 78/210 [00:42<01:10,  1.86it/s]\u001b[A\n 38%|███▊      | 79/210 [00:43<01:10,  1.86it/s]\u001b[A\n 38%|███▊      | 80/210 [00:43<01:09,  1.86it/s]\u001b[A\n 39%|███▊      | 81/210 [00:44<01:09,  1.86it/s]\u001b[A\n 39%|███▉      | 82/210 [00:44<01:08,  1.86it/s]\u001b[A\n 40%|███▉      | 83/210 [00:45<01:10,  1.81it/s]\u001b[A\n 40%|████      | 84/210 [00:45<01:09,  1.82it/s]\u001b[A\n 40%|████      | 85/210 [00:46<01:08,  1.83it/s]\u001b[A\n 41%|████      | 86/210 [00:46<01:07,  1.84it/s]\u001b[A\n 41%|████▏     | 87/210 [00:47<01:06,  1.84it/s]\u001b[A\n 42%|████▏     | 88/210 [00:47<01:06,  1.84it/s]\u001b[A\n 42%|████▏     | 89/210 [00:48<01:05,  1.85it/s]\u001b[A\n 43%|████▎     | 90/210 [00:48<01:04,  1.85it/s]\u001b[A\n 43%|████▎     | 91/210 [00:49<01:04,  1.85it/s]\u001b[A\n 44%|████▍     | 92/210 [00:50<01:03,  1.85it/s]\u001b[A\n 44%|████▍     | 93/210 [00:50<01:03,  1.85it/s]\u001b[A\n 45%|████▍     | 94/210 [00:51<01:02,  1.85it/s]\u001b[A\n 45%|████▌     | 95/210 [00:51<01:01,  1.86it/s]\u001b[A\n 46%|████▌     | 96/210 [00:52<01:01,  1.85it/s]\u001b[A\n 46%|████▌     | 97/210 [00:52<01:01,  1.85it/s]\u001b[A\n 47%|████▋     | 98/210 [00:53<01:00,  1.86it/s]\u001b[A\n 47%|████▋     | 99/210 [00:53<00:59,  1.86it/s]\u001b[A\n 48%|████▊     | 100/210 [00:54<00:59,  1.86it/s]\u001b[A\n 48%|████▊     | 101/210 [00:54<00:58,  1.85it/s]\u001b[A\n 49%|████▊     | 102/210 [00:55<00:58,  1.85it/s]\u001b[A\n 49%|████▉     | 103/210 [00:55<00:57,  1.85it/s]\u001b[A\n 50%|████▉     | 104/210 [00:56<00:57,  1.85it/s]\u001b[A\n 50%|█████     | 105/210 [00:57<00:56,  1.85it/s]\u001b[A\n 50%|█████     | 106/210 [00:57<00:56,  1.86it/s]\u001b[A\n 51%|█████     | 107/210 [00:58<00:55,  1.86it/s]\u001b[A\n 51%|█████▏    | 108/210 [00:58<00:55,  1.85it/s]\u001b[A\n 52%|█████▏    | 109/210 [00:59<00:54,  1.85it/s]\u001b[A\n 52%|█████▏    | 110/210 [00:59<00:54,  1.85it/s]\u001b[A\n 53%|█████▎    | 111/210 [01:00<00:53,  1.85it/s]\u001b[A\n 53%|█████▎    | 112/210 [01:00<00:52,  1.86it/s]\u001b[A\n 54%|█████▍    | 113/210 [01:01<00:52,  1.86it/s]\u001b[A\n 54%|█████▍    | 114/210 [01:01<00:51,  1.86it/s]\u001b[A\n 55%|█████▍    | 115/210 [01:02<00:51,  1.86it/s]\u001b[A\n 55%|█████▌    | 116/210 [01:03<00:50,  1.85it/s]\u001b[A\n 56%|█████▌    | 117/210 [01:03<00:50,  1.85it/s]\u001b[A\n 56%|█████▌    | 118/210 [01:04<00:49,  1.85it/s]\u001b[A\n 57%|█████▋    | 119/210 [01:04<00:49,  1.85it/s]\u001b[A\n 57%|█████▋    | 120/210 [01:05<00:48,  1.85it/s]\u001b[A\n 58%|█████▊    | 121/210 [01:05<00:47,  1.85it/s]\u001b[A\n 58%|█████▊    | 122/210 [01:06<00:47,  1.86it/s]\u001b[A\n 59%|█████▊    | 123/210 [01:06<00:46,  1.85it/s]\u001b[A\n 59%|█████▉    | 124/210 [01:07<00:46,  1.86it/s]\u001b[A\n 60%|█████▉    | 125/210 [01:07<00:45,  1.85it/s]\u001b[A\n 60%|██████    | 126/210 [01:08<00:45,  1.85it/s]\u001b[A\n 60%|██████    | 127/210 [01:08<00:44,  1.86it/s]\u001b[A\n 61%|██████    | 128/210 [01:09<00:44,  1.86it/s]\u001b[A\n 61%|██████▏   | 129/210 [01:10<00:43,  1.85it/s]\u001b[A\n 62%|██████▏   | 130/210 [01:10<00:43,  1.85it/s]\u001b[A\n 62%|██████▏   | 131/210 [01:11<00:42,  1.85it/s]\u001b[A\n 63%|██████▎   | 132/210 [01:11<00:42,  1.85it/s]\u001b[A\n 63%|██████▎   | 133/210 [01:12<00:41,  1.86it/s]\u001b[A\n 64%|██████▍   | 134/210 [01:12<00:40,  1.86it/s]\u001b[A\n 64%|██████▍   | 135/210 [01:13<00:40,  1.86it/s]\u001b[A\n 65%|██████▍   | 136/210 [01:13<00:39,  1.86it/s]\u001b[A\n 65%|██████▌   | 137/210 [01:14<00:39,  1.85it/s]\u001b[A\n 66%|██████▌   | 138/210 [01:14<00:38,  1.86it/s]\u001b[A\n 66%|██████▌   | 139/210 [01:15<00:38,  1.86it/s]\u001b[A\n 67%|██████▋   | 140/210 [01:15<00:37,  1.86it/s]\u001b[A\n 67%|██████▋   | 141/210 [01:16<00:37,  1.85it/s]\u001b[A\n 68%|██████▊   | 142/210 [01:17<00:36,  1.85it/s]\u001b[A\n 68%|██████▊   | 143/210 [01:17<00:36,  1.85it/s]\u001b[A\n 69%|██████▊   | 144/210 [01:18<00:35,  1.85it/s]\u001b[A\n 69%|██████▉   | 145/210 [01:18<00:35,  1.86it/s]\u001b[A\n 70%|██████▉   | 146/210 [01:19<00:34,  1.86it/s]\u001b[A\n 70%|███████   | 147/210 [01:19<00:34,  1.85it/s]\u001b[A\n 70%|███████   | 148/210 [01:20<00:33,  1.85it/s]\u001b[A\n 71%|███████   | 149/210 [01:20<00:32,  1.85it/s]\u001b[A\n 71%|███████▏  | 150/210 [01:21<00:32,  1.85it/s]\u001b[A\n 72%|███████▏  | 151/210 [01:21<00:31,  1.86it/s]\u001b[A\n 72%|███████▏  | 152/210 [01:22<00:31,  1.86it/s]\u001b[A\n 73%|███████▎  | 153/210 [01:22<00:30,  1.86it/s]\u001b[A\n 73%|███████▎  | 154/210 [01:23<00:30,  1.86it/s]\u001b[A\n 74%|███████▍  | 155/210 [01:24<00:29,  1.86it/s]\u001b[A\n 74%|███████▍  | 156/210 [01:24<00:28,  1.87it/s]\u001b[A\n 75%|███████▍  | 157/210 [01:25<00:28,  1.86it/s]\u001b[A\n 75%|███████▌  | 158/210 [01:25<00:27,  1.86it/s]\u001b[A\n 76%|███████▌  | 159/210 [01:26<00:27,  1.87it/s]\u001b[A\n 76%|███████▌  | 160/210 [01:26<00:26,  1.87it/s]\u001b[A\n 77%|███████▋  | 161/210 [01:27<00:26,  1.86it/s]\u001b[A\n 77%|███████▋  | 162/210 [01:27<00:25,  1.86it/s]\u001b[A\n 78%|███████▊  | 163/210 [01:28<00:25,  1.86it/s]\u001b[A\n 78%|███████▊  | 164/210 [01:28<00:25,  1.80it/s]\u001b[A\n 79%|███████▊  | 165/210 [01:29<00:24,  1.83it/s]\u001b[A\n 79%|███████▉  | 166/210 [01:29<00:23,  1.84it/s]\u001b[A\n 80%|███████▉  | 167/210 [01:30<00:23,  1.85it/s]\u001b[A\n 80%|████████  | 168/210 [01:31<00:22,  1.85it/s]\u001b[A\n 80%|████████  | 169/210 [01:31<00:22,  1.85it/s]\u001b[A\n 81%|████████  | 170/210 [01:32<00:21,  1.85it/s]\u001b[A\n 81%|████████▏ | 171/210 [01:32<00:21,  1.85it/s]\u001b[A\n 82%|████████▏ | 172/210 [01:33<00:20,  1.85it/s]\u001b[A\n 82%|████████▏ | 173/210 [01:33<00:20,  1.85it/s]\u001b[A\n 83%|████████▎ | 174/210 [01:34<00:19,  1.84it/s]\u001b[A\n 83%|████████▎ | 175/210 [01:34<00:18,  1.85it/s]\u001b[A\n 84%|████████▍ | 176/210 [01:35<00:18,  1.85it/s]\u001b[A\n 84%|████████▍ | 177/210 [01:35<00:17,  1.85it/s]\u001b[A\n 85%|████████▍ | 178/210 [01:36<00:17,  1.85it/s]\u001b[A\n 85%|████████▌ | 179/210 [01:37<00:16,  1.85it/s]\u001b[A\n 86%|████████▌ | 180/210 [01:37<00:16,  1.85it/s]\u001b[A\n 86%|████████▌ | 181/210 [01:38<00:15,  1.85it/s]\u001b[A\n 87%|████████▋ | 182/210 [01:38<00:15,  1.86it/s]\u001b[A\n 87%|████████▋ | 183/210 [01:39<00:14,  1.86it/s]\u001b[A\n 88%|████████▊ | 184/210 [01:39<00:14,  1.85it/s]\u001b[A\n 88%|████████▊ | 185/210 [01:40<00:13,  1.86it/s]\u001b[A\n 89%|████████▊ | 186/210 [01:40<00:12,  1.86it/s]\u001b[A\n 89%|████████▉ | 187/210 [01:41<00:12,  1.86it/s]\u001b[A\n 90%|████████▉ | 188/210 [01:41<00:11,  1.86it/s]\u001b[A\n 90%|█████████ | 189/210 [01:42<00:11,  1.86it/s]\u001b[A\n 90%|█████████ | 190/210 [01:42<00:10,  1.86it/s]\u001b[A\n 91%|█████████ | 191/210 [01:43<00:10,  1.86it/s]\u001b[A\n 91%|█████████▏| 192/210 [01:44<00:09,  1.86it/s]\u001b[A\n 92%|█████████▏| 193/210 [01:44<00:09,  1.86it/s]\u001b[A\n 92%|█████████▏| 194/210 [01:45<00:08,  1.86it/s]\u001b[A\n 93%|█████████▎| 195/210 [01:45<00:08,  1.86it/s]\u001b[A\n 93%|█████████▎| 196/210 [01:46<00:07,  1.86it/s]\u001b[A\n 94%|█████████▍| 197/210 [01:46<00:07,  1.86it/s]\u001b[A\n 94%|█████████▍| 198/210 [01:47<00:06,  1.85it/s]\u001b[A\n 95%|█████████▍| 199/210 [01:47<00:05,  1.85it/s]\u001b[A\n 95%|█████████▌| 200/210 [01:48<00:05,  1.86it/s]\u001b[A\n 96%|█████████▌| 201/210 [01:48<00:04,  1.85it/s]\u001b[A\n 96%|█████████▌| 202/210 [01:49<00:04,  1.85it/s]\u001b[A\n 97%|█████████▋| 203/210 [01:49<00:03,  1.85it/s]\u001b[A\n 97%|█████████▋| 204/210 [01:50<00:03,  1.86it/s]\u001b[A\n 98%|█████████▊| 205/210 [01:51<00:02,  1.86it/s]\u001b[A\n 98%|█████████▊| 206/210 [01:51<00:02,  1.86it/s]\u001b[A\n 99%|█████████▊| 207/210 [01:52<00:01,  1.86it/s]\u001b[A\n 99%|█████████▉| 208/210 [01:52<00:01,  1.86it/s]\u001b[A\n100%|█████████▉| 209/210 [01:53<00:00,  1.86it/s]\u001b[A\n100%|██████████| 210/210 [01:53<00:00,  1.85it/s]\u001b[A\n 60%|██████    | 12500/20721 [6:18:30<82:06:12, 35.95s/it]","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.3703, Validation Accuracy: 88.16%\n","output_type":"stream"},{"name":"stderr","text":" 63%|██████▎   | 13000/20721 [6:33:28<3:51:20,  1.80s/it] ","output_type":"stream"},{"name":"stdout","text":"Step 13000/20721: Avg Train Loss = 0.3699, Train Accuracy = 100.00%\n","output_type":"stream"},{"name":"stderr","text":" 65%|██████▌   | 13500/20721 [6:48:26<3:36:43,  1.80s/it]","output_type":"stream"},{"name":"stdout","text":"Step 13500/20721: Avg Train Loss = 0.3832, Train Accuracy = 75.00%\n","output_type":"stream"},{"name":"stderr","text":" 68%|██████▊   | 14000/20721 [7:03:24<3:21:21,  1.80s/it]","output_type":"stream"},{"name":"stdout","text":"Step 14000/20721: Avg Train Loss = 0.3798, Train Accuracy = 87.50%\n","output_type":"stream"},{"name":"stderr","text":" 70%|██████▉   | 14500/20721 [7:18:22<3:06:12,  1.80s/it]","output_type":"stream"},{"name":"stdout","text":"Step 14500/20721: Avg Train Loss = 0.3892, Train Accuracy = 100.00%\n","output_type":"stream"},{"name":"stderr","text":" 72%|███████▏  | 15000/20721 [7:33:20<2:52:55,  1.81s/it]","output_type":"stream"},{"name":"stdout","text":"Step 15000/20721: Avg Train Loss = 0.3846, Train Accuracy = 100.00%\n","output_type":"stream"},{"name":"stderr","text":" 75%|███████▍  | 15500/20721 [7:48:19<2:36:06,  1.79s/it]","output_type":"stream"},{"name":"stdout","text":"Step 15500/20721: Avg Train Loss = 0.3679, Train Accuracy = 100.00%\n","output_type":"stream"},{"name":"stderr","text":" 77%|███████▋  | 16000/20721 [8:03:18<2:21:30,  1.80s/it]","output_type":"stream"},{"name":"stdout","text":"Step 16000/20721: Avg Train Loss = 0.3795, Train Accuracy = 87.50%\n","output_type":"stream"},{"name":"stderr","text":" 80%|███████▉  | 16500/20721 [8:18:17<2:06:24,  1.80s/it]","output_type":"stream"},{"name":"stdout","text":"Step 16500/20721: Avg Train Loss = 0.3772, Train Accuracy = 75.00%\n","output_type":"stream"},{"name":"stderr","text":" 82%|████████▏ | 17000/20721 [8:33:15<1:51:33,  1.80s/it]","output_type":"stream"},{"name":"stdout","text":"Step 17000/20721: Avg Train Loss = 0.3721, Train Accuracy = 100.00%\n","output_type":"stream"},{"name":"stderr","text":" 84%|████████▍ | 17500/20721 [8:48:13<1:36:43,  1.80s/it]","output_type":"stream"},{"name":"stdout","text":"Step 17500/20721: Avg Train Loss = 0.3776, Train Accuracy = 87.50%\n","output_type":"stream"},{"name":"stderr","text":" 87%|████████▋ | 18000/20721 [9:03:12<1:21:34,  1.80s/it]","output_type":"stream"},{"name":"stdout","text":"Step 18000/20721: Avg Train Loss = 0.3692, Train Accuracy = 87.50%\n","output_type":"stream"},{"name":"stderr","text":" 89%|████████▉ | 18500/20721 [9:18:10<1:06:07,  1.79s/it]","output_type":"stream"},{"name":"stdout","text":"Step 18500/20721: Avg Train Loss = 0.3571, Train Accuracy = 100.00%\n","output_type":"stream"},{"name":"stderr","text":" 90%|█████████ | 18749/20721 [9:25:37<59:06,  1.80s/it]  \n  0%|          | 0/210 [00:00<?, ?it/s]\u001b[Ahuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n\n  0%|          | 1/210 [00:00<02:54,  1.20it/s]\u001b[A\n  1%|          | 2/210 [00:01<02:17,  1.51it/s]\u001b[A\n  1%|▏         | 3/210 [00:01<02:05,  1.65it/s]\u001b[A\n  2%|▏         | 4/210 [00:02<01:59,  1.72it/s]\u001b[A\n  2%|▏         | 5/210 [00:03<01:56,  1.76it/s]\u001b[A\n  3%|▎         | 6/210 [00:03<01:54,  1.79it/s]\u001b[A\n  3%|▎         | 7/210 [00:04<01:52,  1.81it/s]\u001b[A\n  4%|▍         | 8/210 [00:04<01:50,  1.82it/s]\u001b[A\n  4%|▍         | 9/210 [00:05<01:49,  1.83it/s]\u001b[A\n  5%|▍         | 10/210 [00:05<01:48,  1.84it/s]\u001b[A\n  5%|▌         | 11/210 [00:06<01:47,  1.85it/s]\u001b[A\n  6%|▌         | 12/210 [00:06<01:47,  1.85it/s]\u001b[A\n  6%|▌         | 13/210 [00:07<01:46,  1.85it/s]\u001b[A\n  7%|▋         | 14/210 [00:07<01:45,  1.85it/s]\u001b[A\n  7%|▋         | 15/210 [00:08<01:45,  1.86it/s]\u001b[A\n  8%|▊         | 16/210 [00:08<01:44,  1.86it/s]\u001b[A\n  8%|▊         | 17/210 [00:09<01:43,  1.86it/s]\u001b[A\n  9%|▊         | 18/210 [00:10<01:43,  1.86it/s]\u001b[A\n  9%|▉         | 19/210 [00:10<01:42,  1.86it/s]\u001b[A\n 10%|▉         | 20/210 [00:11<01:42,  1.86it/s]\u001b[A\n 10%|█         | 21/210 [00:11<01:41,  1.85it/s]\u001b[A\n 10%|█         | 22/210 [00:12<01:41,  1.85it/s]\u001b[A\n 11%|█         | 23/210 [00:12<01:40,  1.85it/s]\u001b[A\n 11%|█▏        | 24/210 [00:13<01:40,  1.85it/s]\u001b[A\n 12%|█▏        | 25/210 [00:13<01:39,  1.85it/s]\u001b[A\n 12%|█▏        | 26/210 [00:14<01:39,  1.85it/s]\u001b[A\n 13%|█▎        | 27/210 [00:14<01:38,  1.85it/s]\u001b[A\n 13%|█▎        | 28/210 [00:15<01:38,  1.86it/s]\u001b[A\n 14%|█▍        | 29/210 [00:15<01:37,  1.86it/s]\u001b[A\n 14%|█▍        | 30/210 [00:16<01:37,  1.85it/s]\u001b[A\n 15%|█▍        | 31/210 [00:17<01:36,  1.85it/s]\u001b[A\n 15%|█▌        | 32/210 [00:17<01:35,  1.85it/s]\u001b[A\n 16%|█▌        | 33/210 [00:18<01:35,  1.85it/s]\u001b[A\n 16%|█▌        | 34/210 [00:18<01:34,  1.86it/s]\u001b[A\n 17%|█▋        | 35/210 [00:19<01:44,  1.68it/s]\u001b[A\n 17%|█▋        | 36/210 [00:19<01:40,  1.73it/s]\u001b[A\n 18%|█▊        | 37/210 [00:20<01:38,  1.76it/s]\u001b[A\n 18%|█▊        | 38/210 [00:20<01:36,  1.79it/s]\u001b[A\n 19%|█▊        | 39/210 [00:21<01:34,  1.81it/s]\u001b[A\n 19%|█▉        | 40/210 [00:22<01:33,  1.82it/s]\u001b[A\n 20%|█▉        | 41/210 [00:22<01:32,  1.83it/s]\u001b[A\n 20%|██        | 42/210 [00:23<01:31,  1.84it/s]\u001b[A\n 20%|██        | 43/210 [00:23<01:30,  1.84it/s]\u001b[A\n 21%|██        | 44/210 [00:24<01:30,  1.84it/s]\u001b[A\n 21%|██▏       | 45/210 [00:24<01:29,  1.85it/s]\u001b[A\n 22%|██▏       | 46/210 [00:25<01:28,  1.85it/s]\u001b[A\n 22%|██▏       | 47/210 [00:25<01:28,  1.85it/s]\u001b[A\n 23%|██▎       | 48/210 [00:26<01:27,  1.85it/s]\u001b[A\n 23%|██▎       | 49/210 [00:26<01:26,  1.85it/s]\u001b[A\n 24%|██▍       | 50/210 [00:27<01:26,  1.85it/s]\u001b[A\n 24%|██▍       | 51/210 [00:27<01:25,  1.85it/s]\u001b[A\n 25%|██▍       | 52/210 [00:28<01:25,  1.86it/s]\u001b[A\n 25%|██▌       | 53/210 [00:29<01:24,  1.86it/s]\u001b[A\n 26%|██▌       | 54/210 [00:29<01:23,  1.86it/s]\u001b[A\n 26%|██▌       | 55/210 [00:30<01:23,  1.86it/s]\u001b[A\n 27%|██▋       | 56/210 [00:30<01:22,  1.86it/s]\u001b[A\n 27%|██▋       | 57/210 [00:31<01:22,  1.86it/s]\u001b[A\n 28%|██▊       | 58/210 [00:31<01:21,  1.86it/s]\u001b[A\n 28%|██▊       | 59/210 [00:32<01:21,  1.86it/s]\u001b[A\n 29%|██▊       | 60/210 [00:32<01:20,  1.85it/s]\u001b[A\n 29%|██▉       | 61/210 [00:33<01:20,  1.86it/s]\u001b[A\n 30%|██▉       | 62/210 [00:33<01:19,  1.86it/s]\u001b[A\n 30%|███       | 63/210 [00:34<01:19,  1.86it/s]\u001b[A\n 30%|███       | 64/210 [00:35<01:18,  1.85it/s]\u001b[A\n 31%|███       | 65/210 [00:35<01:18,  1.85it/s]\u001b[A\n 31%|███▏      | 66/210 [00:36<01:17,  1.85it/s]\u001b[A\n 32%|███▏      | 67/210 [00:36<01:17,  1.85it/s]\u001b[A\n 32%|███▏      | 68/210 [00:37<01:16,  1.85it/s]\u001b[A\n 33%|███▎      | 69/210 [00:37<01:16,  1.85it/s]\u001b[A\n 33%|███▎      | 70/210 [00:38<01:15,  1.85it/s]\u001b[A\n 34%|███▍      | 71/210 [00:38<01:15,  1.85it/s]\u001b[A\n 34%|███▍      | 72/210 [00:39<01:14,  1.85it/s]\u001b[A\n 35%|███▍      | 73/210 [00:39<01:14,  1.84it/s]\u001b[A\n 35%|███▌      | 74/210 [00:40<01:13,  1.85it/s]\u001b[A\n 36%|███▌      | 75/210 [00:40<01:13,  1.85it/s]\u001b[A\n 36%|███▌      | 76/210 [00:41<01:12,  1.85it/s]\u001b[A\n 37%|███▋      | 77/210 [00:42<01:11,  1.85it/s]\u001b[A\n 37%|███▋      | 78/210 [00:42<01:11,  1.85it/s]\u001b[A\n 38%|███▊      | 79/210 [00:43<01:10,  1.85it/s]\u001b[A\n 38%|███▊      | 80/210 [00:43<01:10,  1.85it/s]\u001b[A\n 39%|███▊      | 81/210 [00:44<01:09,  1.85it/s]\u001b[A\n 39%|███▉      | 82/210 [00:44<01:08,  1.86it/s]\u001b[A\n 40%|███▉      | 83/210 [00:45<01:08,  1.85it/s]\u001b[A\n 40%|████      | 84/210 [00:45<01:08,  1.85it/s]\u001b[A\n 40%|████      | 85/210 [00:46<01:07,  1.85it/s]\u001b[A\n 41%|████      | 86/210 [00:46<01:06,  1.85it/s]\u001b[A\n 41%|████▏     | 87/210 [00:47<01:06,  1.85it/s]\u001b[A\n 42%|████▏     | 88/210 [00:47<01:06,  1.84it/s]\u001b[A\n 42%|████▏     | 89/210 [00:48<01:05,  1.85it/s]\u001b[A\n 43%|████▎     | 90/210 [00:49<01:04,  1.85it/s]\u001b[A\n 43%|████▎     | 91/210 [00:49<01:04,  1.85it/s]\u001b[A\n 44%|████▍     | 92/210 [00:50<01:03,  1.85it/s]\u001b[A\n 44%|████▍     | 93/210 [00:50<01:03,  1.85it/s]\u001b[A\n 45%|████▍     | 94/210 [00:51<01:02,  1.84it/s]\u001b[A\n 45%|████▌     | 95/210 [00:51<01:02,  1.85it/s]\u001b[A\n 46%|████▌     | 96/210 [00:52<01:01,  1.85it/s]\u001b[A\n 46%|████▌     | 97/210 [00:52<01:01,  1.85it/s]\u001b[A\n 47%|████▋     | 98/210 [00:53<01:00,  1.85it/s]\u001b[A\n 47%|████▋     | 99/210 [00:53<00:59,  1.85it/s]\u001b[A\n 48%|████▊     | 100/210 [00:54<00:59,  1.85it/s]\u001b[A\n 48%|████▊     | 101/210 [00:54<00:58,  1.85it/s]\u001b[A\n 49%|████▊     | 102/210 [00:55<00:58,  1.85it/s]\u001b[A\n 49%|████▉     | 103/210 [00:56<00:57,  1.85it/s]\u001b[A\n 50%|████▉     | 104/210 [00:56<00:57,  1.85it/s]\u001b[A\n 50%|█████     | 105/210 [00:57<00:56,  1.85it/s]\u001b[A\n 50%|█████     | 106/210 [00:57<00:56,  1.85it/s]\u001b[A\n 51%|█████     | 107/210 [00:58<00:55,  1.85it/s]\u001b[A\n 51%|█████▏    | 108/210 [00:58<00:54,  1.86it/s]\u001b[A\n 52%|█████▏    | 109/210 [00:59<00:54,  1.86it/s]\u001b[A\n 52%|█████▏    | 110/210 [00:59<00:53,  1.86it/s]\u001b[A\n 53%|█████▎    | 111/210 [01:00<00:53,  1.86it/s]\u001b[A\n 53%|█████▎    | 112/210 [01:00<00:52,  1.86it/s]\u001b[A\n 54%|█████▍    | 113/210 [01:01<00:52,  1.86it/s]\u001b[A\n 54%|█████▍    | 114/210 [01:02<00:51,  1.86it/s]\u001b[A\n 55%|█████▍    | 115/210 [01:02<00:51,  1.85it/s]\u001b[A\n 55%|█████▌    | 116/210 [01:03<00:50,  1.85it/s]\u001b[A\n 56%|█████▌    | 117/210 [01:03<00:50,  1.85it/s]\u001b[A\n 56%|█████▌    | 118/210 [01:04<00:49,  1.85it/s]\u001b[A\n 57%|█████▋    | 119/210 [01:04<00:49,  1.85it/s]\u001b[A\n 57%|█████▋    | 120/210 [01:05<00:48,  1.85it/s]\u001b[A\n 58%|█████▊    | 121/210 [01:05<00:48,  1.85it/s]\u001b[A\n 58%|█████▊    | 122/210 [01:06<00:47,  1.85it/s]\u001b[A\n 59%|█████▊    | 123/210 [01:06<00:46,  1.85it/s]\u001b[A\n 59%|█████▉    | 124/210 [01:07<00:46,  1.85it/s]\u001b[A\n 60%|█████▉    | 125/210 [01:07<00:45,  1.85it/s]\u001b[A\n 60%|██████    | 126/210 [01:08<00:45,  1.85it/s]\u001b[A\n 60%|██████    | 127/210 [01:09<00:44,  1.85it/s]\u001b[A\n 61%|██████    | 128/210 [01:09<00:44,  1.85it/s]\u001b[A\n 61%|██████▏   | 129/210 [01:10<00:43,  1.85it/s]\u001b[A\n 62%|██████▏   | 130/210 [01:10<00:43,  1.85it/s]\u001b[A\n 62%|██████▏   | 131/210 [01:11<00:42,  1.85it/s]\u001b[A\n 63%|██████▎   | 132/210 [01:11<00:42,  1.85it/s]\u001b[A\n 63%|██████▎   | 133/210 [01:12<00:41,  1.85it/s]\u001b[A\n 64%|██████▍   | 134/210 [01:12<00:40,  1.85it/s]\u001b[A\n 64%|██████▍   | 135/210 [01:13<00:41,  1.80it/s]\u001b[A\n 65%|██████▍   | 136/210 [01:13<00:40,  1.82it/s]\u001b[A\n 65%|██████▌   | 137/210 [01:14<00:39,  1.83it/s]\u001b[A\n 66%|██████▌   | 138/210 [01:15<00:39,  1.83it/s]\u001b[A\n 66%|██████▌   | 139/210 [01:15<00:38,  1.84it/s]\u001b[A\n 67%|██████▋   | 140/210 [01:16<00:38,  1.84it/s]\u001b[A\n 67%|██████▋   | 141/210 [01:16<00:37,  1.85it/s]\u001b[A\n 68%|██████▊   | 142/210 [01:17<00:36,  1.84it/s]\u001b[A\n 68%|██████▊   | 143/210 [01:17<00:36,  1.84it/s]\u001b[A\n 69%|██████▊   | 144/210 [01:18<00:35,  1.84it/s]\u001b[A\n 69%|██████▉   | 145/210 [01:18<00:35,  1.85it/s]\u001b[A\n 70%|██████▉   | 146/210 [01:19<00:34,  1.84it/s]\u001b[A\n 70%|███████   | 147/210 [01:19<00:34,  1.84it/s]\u001b[A\n 70%|███████   | 148/210 [01:20<00:33,  1.85it/s]\u001b[A\n 71%|███████   | 149/210 [01:20<00:33,  1.85it/s]\u001b[A\n 71%|███████▏  | 150/210 [01:21<00:32,  1.85it/s]\u001b[A\n 72%|███████▏  | 151/210 [01:22<00:31,  1.85it/s]\u001b[A\n 72%|███████▏  | 152/210 [01:22<00:31,  1.86it/s]\u001b[A\n 73%|███████▎  | 153/210 [01:23<00:30,  1.86it/s]\u001b[A\n 73%|███████▎  | 154/210 [01:23<00:30,  1.86it/s]\u001b[A\n 74%|███████▍  | 155/210 [01:24<00:29,  1.86it/s]\u001b[A\n 74%|███████▍  | 156/210 [01:24<00:28,  1.86it/s]\u001b[A\n 75%|███████▍  | 157/210 [01:25<00:28,  1.86it/s]\u001b[A\n 75%|███████▌  | 158/210 [01:25<00:27,  1.86it/s]\u001b[A\n 76%|███████▌  | 159/210 [01:26<00:27,  1.86it/s]\u001b[A\n 76%|███████▌  | 160/210 [01:26<00:26,  1.86it/s]\u001b[A\n 77%|███████▋  | 161/210 [01:27<00:26,  1.86it/s]\u001b[A\n 77%|███████▋  | 162/210 [01:27<00:25,  1.86it/s]\u001b[A\n 78%|███████▊  | 163/210 [01:28<00:25,  1.86it/s]\u001b[A\n 78%|███████▊  | 164/210 [01:29<00:24,  1.86it/s]\u001b[A\n 79%|███████▊  | 165/210 [01:29<00:24,  1.86it/s]\u001b[A\n 79%|███████▉  | 166/210 [01:30<00:23,  1.86it/s]\u001b[A\n 80%|███████▉  | 167/210 [01:30<00:23,  1.85it/s]\u001b[A\n 80%|████████  | 168/210 [01:31<00:22,  1.85it/s]\u001b[A\n 80%|████████  | 169/210 [01:31<00:22,  1.85it/s]\u001b[A\n 81%|████████  | 170/210 [01:32<00:21,  1.85it/s]\u001b[A\n 81%|████████▏ | 171/210 [01:32<00:21,  1.85it/s]\u001b[A\n 82%|████████▏ | 172/210 [01:33<00:20,  1.85it/s]\u001b[A\n 82%|████████▏ | 173/210 [01:33<00:19,  1.85it/s]\u001b[A\n 83%|████████▎ | 174/210 [01:34<00:19,  1.86it/s]\u001b[A\n 83%|████████▎ | 175/210 [01:34<00:18,  1.85it/s]\u001b[A\n 84%|████████▍ | 176/210 [01:35<00:18,  1.85it/s]\u001b[A\n 84%|████████▍ | 177/210 [01:36<00:17,  1.85it/s]\u001b[A\n 85%|████████▍ | 178/210 [01:36<00:17,  1.85it/s]\u001b[A\n 85%|████████▌ | 179/210 [01:37<00:16,  1.85it/s]\u001b[A\n 86%|████████▌ | 180/210 [01:37<00:16,  1.85it/s]\u001b[A\n 86%|████████▌ | 181/210 [01:38<00:15,  1.85it/s]\u001b[A\n 87%|████████▋ | 182/210 [01:38<00:15,  1.85it/s]\u001b[A\n 87%|████████▋ | 183/210 [01:39<00:14,  1.85it/s]\u001b[A\n 88%|████████▊ | 184/210 [01:39<00:14,  1.85it/s]\u001b[A\n 88%|████████▊ | 185/210 [01:40<00:13,  1.85it/s]\u001b[A\n 89%|████████▊ | 186/210 [01:40<00:12,  1.86it/s]\u001b[A\n 89%|████████▉ | 187/210 [01:41<00:12,  1.85it/s]\u001b[A\n 90%|████████▉ | 188/210 [01:42<00:11,  1.85it/s]\u001b[A\n 90%|█████████ | 189/210 [01:42<00:11,  1.85it/s]\u001b[A\n 90%|█████████ | 190/210 [01:43<00:10,  1.85it/s]\u001b[A\n 91%|█████████ | 191/210 [01:43<00:10,  1.86it/s]\u001b[A\n 91%|█████████▏| 192/210 [01:44<00:09,  1.86it/s]\u001b[A\n 92%|█████████▏| 193/210 [01:44<00:09,  1.86it/s]\u001b[A\n 92%|█████████▏| 194/210 [01:45<00:08,  1.86it/s]\u001b[A\n 93%|█████████▎| 195/210 [01:45<00:08,  1.85it/s]\u001b[A\n 93%|█████████▎| 196/210 [01:46<00:07,  1.86it/s]\u001b[A\n 94%|█████████▍| 197/210 [01:46<00:07,  1.85it/s]\u001b[A\n 94%|█████████▍| 198/210 [01:47<00:06,  1.85it/s]\u001b[A\n 95%|█████████▍| 199/210 [01:47<00:05,  1.85it/s]\u001b[A\n 95%|█████████▌| 200/210 [01:48<00:05,  1.85it/s]\u001b[A\n 96%|█████████▌| 201/210 [01:49<00:04,  1.85it/s]\u001b[A\n 96%|█████████▌| 202/210 [01:49<00:04,  1.85it/s]\u001b[A\n 97%|█████████▋| 203/210 [01:50<00:03,  1.86it/s]\u001b[A\n 97%|█████████▋| 204/210 [01:50<00:03,  1.85it/s]\u001b[A\n 98%|█████████▊| 205/210 [01:51<00:02,  1.85it/s]\u001b[A\n 98%|█████████▊| 206/210 [01:51<00:02,  1.86it/s]\u001b[A\n 99%|█████████▊| 207/210 [01:52<00:01,  1.85it/s]\u001b[A\n 99%|█████████▉| 208/210 [01:52<00:01,  1.86it/s]\u001b[A\n100%|█████████▉| 209/210 [01:53<00:00,  1.86it/s]\u001b[A\n100%|██████████| 210/210 [01:53<00:00,  1.84it/s]\u001b[A\n 90%|█████████ | 18750/20721 [9:27:33<19:42:31, 36.00s/it]","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.3511, Validation Accuracy: 90.07%\n","output_type":"stream"},{"name":"stderr","text":" 92%|█████████▏| 19000/20721 [9:35:02<51:30,  1.80s/it]   ","output_type":"stream"},{"name":"stdout","text":"Step 19000/20721: Avg Train Loss = 0.3664, Train Accuracy = 37.50%\n","output_type":"stream"},{"name":"stderr","text":" 94%|█████████▍| 19500/20721 [9:50:01<36:37,  1.80s/it]","output_type":"stream"},{"name":"stdout","text":"Step 19500/20721: Avg Train Loss = 0.3678, Train Accuracy = 100.00%\n","output_type":"stream"},{"name":"stderr","text":" 97%|█████████▋| 20000/20721 [10:04:59<21:33,  1.79s/it]","output_type":"stream"},{"name":"stdout","text":"Step 20000/20721: Avg Train Loss = 0.3767, Train Accuracy = 75.00%\n","output_type":"stream"},{"name":"stderr","text":" 99%|█████████▉| 20500/20721 [10:19:57<06:36,  1.80s/it]","output_type":"stream"},{"name":"stdout","text":"Step 20500/20721: Avg Train Loss = 0.3518, Train Accuracy = 62.50%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 20721/20721 [10:26:35<00:00,  1.81s/it]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[11], line 124\u001b[0m\n\u001b[1;32m    121\u001b[0m         total_rows_processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# Reset after each evaluation\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Save model and reset\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/kaggle/working/Models/Mamba-780m-Step-\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mbatch_index\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m-Loss-\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtest_loss\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBest AUROC: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_auroc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:651\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    648\u001b[0m _check_save_filelike(f)\n\u001b[1;32m    650\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m--> 651\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    652\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\n\u001b[1;32m    653\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:525\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    524\u001b[0m     container \u001b[38;5;241m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[0;32m--> 525\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:496\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mPyTorchFileWriter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream))\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 496\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m)\n","\u001b[0;31mRuntimeError\u001b[0m: Parent directory /kaggle/working/Models does not exist."],"ename":"RuntimeError","evalue":"Parent directory /kaggle/working/Models does not exist.","output_type":"error"}]},{"cell_type":"code","source":"torch.save(model.state_dict(), f'/kaggle/working/Mamba-780m-Step-{batch_index+1}-Loss-{int(test_loss*1000)}.pth')\n","metadata":{"execution":{"iopub.status.busy":"2024-09-20T04:13:25.274045Z","iopub.execute_input":"2024-09-20T04:13:25.274416Z","iopub.status.idle":"2024-09-20T04:13:30.882150Z","shell.execute_reply.started":"2024-09-20T04:13:25.274380Z","shell.execute_reply":"2024-09-20T04:13:30.880422Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"model.eval()\nval_accuracy, test_loss = TestModel(test_data_loader, model, criterion)\nmodel.train()\n\nprint(f'Validation Loss: {test_loss:.4f}, Validation Accuracy: {val_accuracy*100:.2f}%')","metadata":{"execution":{"iopub.status.busy":"2024-09-20T04:14:46.771286Z","iopub.execute_input":"2024-09-20T04:14:46.771669Z","iopub.status.idle":"2024-09-20T04:16:40.585821Z","shell.execute_reply.started":"2024-09-20T04:14:46.771631Z","shell.execute_reply":"2024-09-20T04:16:40.584665Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"  0%|          | 0/210 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n100%|██████████| 210/210 [01:53<00:00,  1.85it/s]","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.3493, Validation Accuracy: 90.18%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"auroc_scores_by_dataset, test_loss","metadata":{"execution":{"iopub.status.busy":"2024-09-20T03:37:05.182217Z","iopub.status.idle":"2024-09-20T03:37:05.182548Z","shell.execute_reply.started":"2024-09-20T03:37:05.182382Z","shell.execute_reply":"2024-09-20T03:37:05.182399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom tqdm import tqdm\nfrom sklearn.metrics import roc_auc_score\n\nmodel.eval()\nauroc_scores_by_dataset, test_loss = TestModel(test_data_loader, model, criterion)\nmodel.train()\n\n# average_auroc = np.average(auroc_scores_by_dataset, weights=[1, 1])\n# if (average_auroc > best_auroc) or (max(auroc_scores_by_dataset) > 0.993):\n#     best_auroc = average_auroc\n#     if output_subdir is not None:\n#         torch.save(model.state_dict(), f'Models/Mamba/{output_subdir}/S{step_number}_CTX1024.pth')\n\n# train_losses = []","metadata":{"execution":{"iopub.status.busy":"2024-09-20T03:37:05.183718Z","iopub.status.idle":"2024-09-20T03:37:05.184069Z","shell.execute_reply.started":"2024-09-20T03:37:05.183903Z","shell.execute_reply":"2024-09-20T03:37:05.183920Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b><span style='color:#F1A424'>Confusion Matrix</span></b>\n","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\nfrom sklearn.metrics import confusion_matrix\n\ndef binarize(x, threshold):\n    if x > threshold:\n        x = 1\n    else:\n        x = 0\n    return x\n\n# Assuming df is your pandas DataFrame\noof_df[\"binary\"] = oof_df[\"preds\"].apply(lambda x: binarize(x, 0.5))\ntrue_labels = oof_df[\"generated\"].values\npredicted_labels = oof_df[\"binary\"].values\n\n# Get the unique classes from both true and predicted labels\nclasses = np.unique(np.concatenate((true_labels, predicted_labels)))\n\n# Compute the confusion matrix\ncm = confusion_matrix(true_labels, predicted_labels, labels=classes)\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=classes, yticklabels=classes)\nplt.xlabel(\"Predicted Labels\")\nplt.ylabel(\"True Labels\")\nplt.title(\"Confusion Matrix\")","metadata":{"execution":{"iopub.status.busy":"2024-09-20T03:37:05.185888Z","iopub.status.idle":"2024-09-20T03:37:05.186360Z","shell.execute_reply.started":"2024-09-20T03:37:05.186109Z","shell.execute_reply":"2024-09-20T03:37:05.186134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}