{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b><span style='color:#F1A424'>|</span> Import Libraries</b><a class='anchor' id='import_libraries'></a> [↑](#top) \n",
    "\n",
    "***\n",
    "\n",
    "Import all the required libraries for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-19T17:05:23.520178Z",
     "iopub.status.busy": "2024-09-19T17:05:23.519758Z",
     "iopub.status.idle": "2024-09-19T17:05:27.482622Z",
     "shell.execute_reply": "2024-09-19T17:05:27.481379Z",
     "shell.execute_reply.started": "2024-09-19T17:05:23.520134Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ea301b/anaconda3/envs/binh_mamba/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device is: cuda\n",
      "mkdir: cannot create directory ‘output’: File exists\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "# import wandb\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ======= OPTIONS =========\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Current device is: {device}\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "!mkdir output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-19T17:05:27.484828Z",
     "iopub.status.busy": "2024-09-19T17:05:27.484129Z",
     "iopub.status.idle": "2024-09-19T17:05:43.959428Z",
     "shell.execute_reply": "2024-09-19T17:05:43.958637Z",
     "shell.execute_reply.started": "2024-09-19T17:05:27.484792Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch.nn as nn\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from collections import namedtuple\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel\n",
    "from mamba_ssm.utils.hf import load_config_hf, load_state_dict_hf\n",
    "# from huggingface_hub import HfApi\n",
    "\n",
    "# import evaluate\n",
    "import numpy as np\n",
    "# from datasets import load_dataset\n",
    "# from transformers import Trainer\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import AutoTokenizer, TrainingArguments\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-18T04:07:22.734063Z",
     "iopub.status.busy": "2024-09-18T04:07:22.733462Z",
     "iopub.status.idle": "2024-09-18T04:07:22.850537Z",
     "shell.execute_reply": "2024-09-18T04:07:22.849644Z",
     "shell.execute_reply.started": "2024-09-18T04:07:22.734029Z"
    },
    "trusted": true
   },
   "source": [
    "import wandb\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"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\")\n",
    "notes = \"Train Mamba With 400k row dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.012589,
     "end_time": "2022-08-31T07:03:04.13341",
     "exception": false,
     "start_time": "2022-08-31T07:03:04.120821",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <b><span style='color:#F1A424'>|</span> Load Data</b><a class='anchor' id='load_data'></a> [↑](#top) \n",
    "\n",
    "***\n",
    "\n",
    "Load data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-19T17:05:43.962860Z",
     "iopub.status.busy": "2024-09-19T17:05:43.962010Z",
     "iopub.status.idle": "2024-09-19T17:05:53.355460Z",
     "shell.execute_reply": "2024-09-19T17:05:53.354541Z",
     "shell.execute_reply.started": "2024-09-19T17:05:43.962813Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Text: 100%|██████████| 165767/165767 [00:03<00:00, 42935.51it/s]\n",
      "Processing Text: 100%|██████████| 1679/1679 [00:00<00:00, 43710.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainging DF Processing\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 165767 entries, 0 to 165766\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count   Dtype \n",
      "---  ------     --------------   ----- \n",
      " 0   id         165767 non-null  object\n",
      " 1   prompt_id  165767 non-null  int64 \n",
      " 2   text       165767 non-null  object\n",
      " 3   generated  165767 non-null  int64 \n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 5.1+ MB\n",
      "None\n",
      "Testing DF Processing\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1679 entries, 0 to 1678\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   id         1679 non-null   object\n",
      " 1   prompt_id  1679 non-null   int64 \n",
      " 2   text       1679 non-null   object\n",
      " 3   generated  1679 non-null   int64 \n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 52.6+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load DataFrame\n",
    "train_df = pd.read_parquet('/home/HardDisk/binh230_intern/Mamba-AI-generated-text-detection/data/Mix-AI-Dataset/train_essays.parquet')\n",
    "valid_df = pd.read_parquet('/home/HardDisk/binh230_intern/Mamba-AI-generated-text-detection/data/Mix-AI-Dataset/valid_essays.parquet')\n",
    "\n",
    "# Define characters to remove\n",
    "char_to_remove = ['{', '£', '\\x97', '¹', 'å', '\\\\', '\\x85', '<', '\\x99', \n",
    "                  'é', ']', '+', 'Ö', '\\xa0', '>', '|', '\\x80', '~', '©', \n",
    "                  '/', '\\x93', '$', 'Ó', '²', '^', ';', '`', 'á', '*', '(', \n",
    "                  '¶', '®', '[', '\\x94', '\\x91', '#', '-', 'ó', ')', '}', '=']\n",
    "\n",
    "# Define preprocessing function\n",
    "def preprocess_text(text, strategy='light'):    \n",
    "    if strategy == \"none\":\n",
    "        text = text\n",
    "    elif strategy == \"light\":\n",
    "        text = text.encode(\"ascii\", \"ignore\").decode('ascii')        \n",
    "        text = text.strip()\n",
    "        text = text.strip(\"\\\"\")\n",
    "        for c in char_to_remove:\n",
    "            text = text.replace(c, \"\")\n",
    "        if text and text[-1] != \".\":\n",
    "            text = text.split(\".\")\n",
    "            text = \".\".join(text[:-1])\n",
    "            text += \".\"\n",
    "    else:\n",
    "        text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('ascii')\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^a-z0-9\\s.,;?!:()\\'\\\"%-]', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply preprocessing with progress bar\n",
    "tqdm.pandas(desc=\"Processing Text\")\n",
    "train_df['text'] = train_df['text'].progress_apply(lambda x: preprocess_text(x, strategy='light'))\n",
    "valid_df['text'] = valid_df['text'].progress_apply(lambda x: preprocess_text(x, strategy='light'))\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(\"Trainging DF Processing\")\n",
    "print(train_df.info())\n",
    "print(\"Testing DF Processing\")\n",
    "print(valid_df.info())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.008127,
     "end_time": "2022-08-31T07:03:11.985369",
     "exception": false,
     "start_time": "2022-08-31T07:03:11.977242",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <b><span style='color:#F1A424'>|</span> Dataset</b><a class='anchor' id='dataset'></a> [↑](#top) \n",
    "\n",
    "***\n",
    "\n",
    "    \n",
    "We need to get the `max_len` from our `tokenizer`. We create a `tqdm` iterator and for each text we extract the tokenized length. Then we get the maximum value and we add 3 for the special tokens `CLS`, `SEP`, `SEP`.\n",
    "\n",
    "- [Hugging Face Padding and Truncation](https://huggingface.co/docs/transformers/pad_truncation): check truncation to `max_length` or `True` (batch max length)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One sample from the dataset should look as following:\n",
    "```python\n",
    "{\n",
    "\t'inputs': {\n",
    "\t\t'input_ids': tensor([1, 279, 883, ..., 0, 0]),\n",
    "\t\t'token_type_ids': tensor([0, 0, 0, ..., 0, 0]),\n",
    "\t\t'attention_mask': tensor([1, 1, 1, ..., 0, 0])\n",
    "\t},\n",
    "\t'label': tensor([0.0]),\n",
    "\t'ids': '000e8c3c7ddb'\n",
    "}\n",
    "```\n",
    "You can check it by running the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import wandb\n",
    "# Định nghĩa tên project để log thông tin quá trình huấn luyện trên wandb\n",
    "os.environ[\"WANDB_PROJECT\"] = \"mamba_LLM_detect_binary_classification\"\n",
    "os.environ[\"WANDB_API_KEY \"] = \"e7432690ce6d9bfdee410567f89d7e38844ed584\"\n",
    "\n",
    "\n",
    "wandb.login()\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"mamba_LLM_detect_binary_classification\",\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": 6e-5,\n",
    "    \"architecture\": \"Mamba-130m-with-Linear-Head\",\n",
    "    \"dataset\": \"Test\",\n",
    "    \"epochs\": 1,\n",
    "    \"lr_scheduler_type\": \"cosine\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.008073,
     "end_time": "2022-08-31T07:03:17.933189",
     "exception": false,
     "start_time": "2022-08-31T07:03:17.925116",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <b><span style='color:#F1A424'>|</span> Model</b><a class='anchor' id='model'></a> [↑](#top) \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-19T17:05:53.357180Z",
     "iopub.status.busy": "2024-09-19T17:05:53.356739Z",
     "iopub.status.idle": "2024-09-19T17:05:53.373217Z",
     "shell.execute_reply": "2024-09-19T17:05:53.372238Z",
     "shell.execute_reply.started": "2024-09-19T17:05:53.357130Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>text</th>\n",
       "      <th>generated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e_ddxvqx5i</td>\n",
       "      <td>0</td>\n",
       "      <td>In recent years, there has been a growing move...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e_hi0yzrcv</td>\n",
       "      <td>0</td>\n",
       "      <td>\\nWhy not cars in our life\\n\\nI have ever met ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e_uesv4xha</td>\n",
       "      <td>0</td>\n",
       "      <td>A car is considered by many a nessecity for ev...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e_2tl5ylwy</td>\n",
       "      <td>0</td>\n",
       "      <td>H\\n\\nello fellow citezens , we are here to inf...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e_s6ci4vj0</td>\n",
       "      <td>0</td>\n",
       "      <td>Have you ever known how if feels not being abl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  prompt_id                                               text  generated\n",
       "0  e_ddxvqx5i          0  In recent years, there has been a growing move...          1\n",
       "1  e_hi0yzrcv          0  \\nWhy not cars in our life\\n\\nI have ever met ...          1\n",
       "2  e_uesv4xha          0  A car is considered by many a nessecity for ev...          1\n",
       "3  e_2tl5ylwy          0  H\\n\\nello fellow citezens , we are here to inf...          0\n",
       "4  e_s6ci4vj0          0  Have you ever known how if feels not being abl...          1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-19T17:05:53.374547Z",
     "iopub.status.busy": "2024-09-19T17:05:53.374259Z",
     "iopub.status.idle": "2024-09-19T17:05:56.661369Z",
     "shell.execute_reply": "2024-09-19T17:05:56.660369Z",
     "shell.execute_reply.started": "2024-09-19T17:05:53.374516Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'prompt_id', 'text', 'labels'],\n",
       "        num_rows: 165767\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'prompt_id', 'text', 'labels'],\n",
       "        num_rows: 1679\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming train_df is your DataFrame with a 'text' column\n",
    "# Convert the 'id' column to a string to avoid ArrowTypeError\n",
    "# df['id'] = df['id'].astype(str)\n",
    "\n",
    "# Rename the 'generated' column to 'labels'\n",
    "train_df.rename(columns={'generated': 'labels'}, inplace=True)\n",
    "valid_df.rename(columns={'generated': 'labels'}, inplace=True)\n",
    "\n",
    "# # Access the train and test datasets\n",
    "# train_dataset, test_dataset = train_test_split(df, test_size=0.05)\n",
    "\n",
    "# Combine the splits into a DatasetDict\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': Dataset.from_pandas(train_df),\n",
    "    'test': Dataset.from_pandas(valid_df),\n",
    "})\n",
    "\n",
    "# Display the first example from each dataset\n",
    "dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-19T17:05:56.662755Z",
     "iopub.status.busy": "2024-09-19T17:05:56.662437Z",
     "iopub.status.idle": "2024-09-19T17:08:50.995345Z",
     "shell.execute_reply": "2024-09-19T17:08:50.994344Z",
     "shell.execute_reply.started": "2024-09-19T17:05:56.662708Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map: 100%|██████████| 165767/165767 [01:53<00:00, 1463.85 examples/s]\n",
      "Map: 100%|██████████| 1679/1679 [00:01<00:00, 1498.50 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neox-20b')\n",
    "# Add eos tokens\n",
    "# tokenizer.eos_token = \"<|endoftext|>\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "def preprocess_function(examples):\n",
    "    # Tokenize the text with truncation\n",
    "    samples = tokenizer(examples['text'], \n",
    "                        truncation=True, \n",
    "                        padding='max_length', \n",
    "                        max_length=1024,         \n",
    "                        return_tensors=\"pt\")\n",
    "    \n",
    "    return samples\n",
    "\n",
    "# Apply preprocessing to the dataset\n",
    "tokenized_dataset = dataset_dict.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'prompt_id', 'text', 'labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 165767\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'prompt_id', 'text', 'labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 1679\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-19T17:08:50.996892Z",
     "iopub.status.busy": "2024-09-19T17:08:50.996548Z",
     "iopub.status.idle": "2024-09-19T17:08:51.011804Z",
     "shell.execute_reply": "2024-09-19T17:08:51.010782Z",
     "shell.execute_reply.started": "2024-09-19T17:08:50.996858Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Set seed cho hàm random\n",
    "random.seed(42)\n",
    "\n",
    "# Tạo tập train và test\n",
    "train_dataset = tokenized_dataset[\"train\"]\n",
    "test_dataset = tokenized_dataset[\"test\"]\n",
    "#  Drop the 'prompt_id' feature from both datasets\n",
    "train_dataset = train_dataset.remove_columns([\"text\"]).remove_columns([\"id\"])\n",
    "test_dataset = test_dataset.remove_columns([\"text\"]).remove_columns([\"id\"])\n",
    "\n",
    "# Tạo tập evaluation để đánh giá trong lúc train\n",
    "# Do số lượng tập test lớn nên chỉ lấy mẫu 1% tập dữ liệu test để đánh giá\n",
    "# total_samples = len(test_dataset)\n",
    "# eval_samples = int(0.5 * total_samples)\n",
    "# eval_indices = random.sample(range(total_samples), eval_samples)\n",
    "# eval_dataset = test_dataset.select(eval_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-19T17:08:51.013411Z",
     "iopub.status.busy": "2024-09-19T17:08:51.013088Z",
     "iopub.status.idle": "2024-09-19T17:10:28.712704Z",
     "shell.execute_reply": "2024-09-19T17:10:28.711832Z",
     "shell.execute_reply.started": "2024-09-19T17:08:51.013380Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "# Load the model\n",
    "import torch\n",
    "from xlstm import (\n",
    "    xLSTMBlockStack,\n",
    "    xLSTMBlockStackConfig,\n",
    "    mLSTMBlockConfig,\n",
    "    mLSTMLayerConfig,\n",
    "    sLSTMBlockConfig,\n",
    "    sLSTMLayerConfig,\n",
    "    FeedForwardConfig,\n",
    ")\n",
    "\n",
    "# Dataset and Tokenizer Setup\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'verbose': True, 'with_cuda': True, 'extra_ldflags': ['-L/home/ea301b/anaconda3/envs/binh_mamba/lib', '-lcublas'], 'extra_cflags': ['-DSLSTM_HIDDEN_SIZE=1024', '-DSLSTM_BATCH_SIZE=8', '-DSLSTM_NUM_HEADS=4', '-DSLSTM_NUM_STATES=4', '-DSLSTM_DTYPE_B=float', '-DSLSTM_DTYPE_R=__nv_bfloat16', '-DSLSTM_DTYPE_W=__nv_bfloat16', '-DSLSTM_DTYPE_G=__nv_bfloat16', '-DSLSTM_DTYPE_S=__nv_bfloat16', '-DSLSTM_DTYPE_A=float', '-DSLSTM_NUM_GATES=4', '-DSLSTM_SIMPLE_AGG=true', '-DSLSTM_GRADIENT_RECURRENT_CLIPVAL_VALID=false', '-DSLSTM_GRADIENT_RECURRENT_CLIPVAL=0.0', '-DSLSTM_FORWARD_CLIPVAL_VALID=false', '-DSLSTM_FORWARD_CLIPVAL=0.0', '-U__CUDA_NO_HALF_OPERATORS__', '-U__CUDA_NO_HALF_CONVERSIONS__', '-U__CUDA_NO_BFLOAT16_OPERATORS__', '-U__CUDA_NO_BFLOAT16_CONVERSIONS__', '-U__CUDA_NO_BFLOAT162_OPERATORS__', '-U__CUDA_NO_BFLOAT162_CONVERSIONS__'], 'extra_cuda_cflags': ['-Xptxas=\"-v\"', '-gencode', 'arch=compute_80,code=compute_80', '-res-usage', '--use_fast_math', '-O3', '-Xptxas -O3', '--extra-device-vectorization', '-DSLSTM_HIDDEN_SIZE=1024', '-DSLSTM_BATCH_SIZE=8', '-DSLSTM_NUM_HEADS=4', '-DSLSTM_NUM_STATES=4', '-DSLSTM_DTYPE_B=float', '-DSLSTM_DTYPE_R=__nv_bfloat16', '-DSLSTM_DTYPE_W=__nv_bfloat16', '-DSLSTM_DTYPE_G=__nv_bfloat16', '-DSLSTM_DTYPE_S=__nv_bfloat16', '-DSLSTM_DTYPE_A=float', '-DSLSTM_NUM_GATES=4', '-DSLSTM_SIMPLE_AGG=true', '-DSLSTM_GRADIENT_RECURRENT_CLIPVAL_VALID=false', '-DSLSTM_GRADIENT_RECURRENT_CLIPVAL=0.0', '-DSLSTM_FORWARD_CLIPVAL_VALID=false', '-DSLSTM_FORWARD_CLIPVAL=0.0', '-U__CUDA_NO_HALF_OPERATORS__', '-U__CUDA_NO_HALF_CONVERSIONS__', '-U__CUDA_NO_BFLOAT16_OPERATORS__', '-U__CUDA_NO_BFLOAT16_CONVERSIONS__', '-U__CUDA_NO_BFLOAT162_OPERATORS__', '-U__CUDA_NO_BFLOAT162_CONVERSIONS__']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/ea301b/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/ea301b/.cache/torch_extensions/py312_cu124/slstm_HS1024BS8NH4NS4DBfDRbDWbDGbDSbDAfNG4SA1GRCV0GRC0d0FCV0FC0d0/build.ninja...\n",
      "Building extension module slstm_HS1024BS8NH4NS4DBfDRbDWbDGbDSbDAfNG4SA1GRCV0GRC0d0FCV0FC0d0...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Loading extension module slstm_HS1024BS8NH4NS4DBfDRbDWbDGbDSbDAfNG4SA1GRCV0GRC0d0FCV0FC0d0...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from omegaconf import OmegaConf\n",
    "from dacite import from_dict\n",
    "from dacite import Config as DaciteConfig\n",
    "from xlstm import xLSTMLMModel, xLSTMLMModelConfig\n",
    "import torch \n",
    "\n",
    "\n",
    "xlstm_cfg = \"\"\" \n",
    "vocab_size: 50304\n",
    "mlstm_block:\n",
    "  mlstm:\n",
    "    conv1d_kernel_size: 4\n",
    "    qkv_proj_blocksize: 4\n",
    "    num_heads: 4\n",
    "slstm_block:\n",
    "  slstm:\n",
    "    backend: cuda\n",
    "    num_heads: 4\n",
    "    conv1d_kernel_size: 4\n",
    "    bias_init: powerlaw_blockdependent\n",
    "  feedforward:\n",
    "    proj_factor: 1.3\n",
    "    act_fn: gelu\n",
    "context_length: 1024\n",
    "num_blocks: 16\n",
    "embedding_dim: 1024\n",
    "slstm_at: [1]\n",
    "\"\"\"\n",
    "cfg = OmegaConf.create(xlstm_cfg)\n",
    "cfg = from_dict(data_class=xLSTMLMModelConfig, data=OmegaConf.to_container(cfg), config=DaciteConfig(strict=True))\n",
    "xlstm_stack = xLSTMLMModel(cfg)\n",
    "\n",
    "x = torch.randint(0, 50304, size=(4, 256)).to(\"cuda\")\n",
    "xlstm_stack = xlstm_stack.to(\"cuda\")\n",
    "y = xlstm_stack(x)\n",
    "y.shape[1:] == (256, 50304)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "xLSTMLMModel(\n",
       "  (xlstm_block_stack): xLSTMBlockStack(\n",
       "    (blocks): ModuleList(\n",
       "      (0): mLSTMBlock(\n",
       "        (xlstm_norm): LayerNorm()\n",
       "        (xlstm): mLSTMLayer(\n",
       "          (proj_up): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "          (q_proj): LinearHeadwiseExpand(in_features=2048, num_heads=512, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
       "          (k_proj): LinearHeadwiseExpand(in_features=2048, num_heads=512, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
       "          (v_proj): LinearHeadwiseExpand(in_features=2048, num_heads=512, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
       "          (conv1d): CausalConv1d(\n",
       "            (conv): Conv1d(2048, 2048, kernel_size=(4,), stride=(1,), padding=(3,), groups=2048)\n",
       "          )\n",
       "          (conv_act_fn): SiLU()\n",
       "          (mlstm_cell): mLSTMCell(\n",
       "            (igate): Linear(in_features=6144, out_features=4, bias=True)\n",
       "            (fgate): Linear(in_features=6144, out_features=4, bias=True)\n",
       "            (outnorm): MultiHeadLayerNorm()\n",
       "          )\n",
       "          (ogate_act_fn): SiLU()\n",
       "          (proj_down): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): sLSTMBlock(\n",
       "        (xlstm_norm): LayerNorm()\n",
       "        (xlstm): sLSTMLayer(\n",
       "          (conv1d): CausalConv1d(\n",
       "            (conv): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)\n",
       "          )\n",
       "          (conv_act_fn): SiLU()\n",
       "          (fgate): LinearHeadwiseExpand(in_features=1024, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
       "          (igate): LinearHeadwiseExpand(in_features=1024, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
       "          (zgate): LinearHeadwiseExpand(in_features=1024, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
       "          (ogate): LinearHeadwiseExpand(in_features=1024, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
       "          (slstm_cell): sLSTMCell_cuda(function=slstm, hidden_size=1024, num_heads=4)\n",
       "          (group_norm): MultiHeadLayerNorm()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ffn_norm): LayerNorm()\n",
       "        (ffn): GatedFeedForward(\n",
       "          (proj_up): Linear(in_features=1024, out_features=2688, bias=False)\n",
       "          (proj_down): Linear(in_features=1344, out_features=1024, bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2-15): 14 x mLSTMBlock(\n",
       "        (xlstm_norm): LayerNorm()\n",
       "        (xlstm): mLSTMLayer(\n",
       "          (proj_up): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "          (q_proj): LinearHeadwiseExpand(in_features=2048, num_heads=512, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
       "          (k_proj): LinearHeadwiseExpand(in_features=2048, num_heads=512, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
       "          (v_proj): LinearHeadwiseExpand(in_features=2048, num_heads=512, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
       "          (conv1d): CausalConv1d(\n",
       "            (conv): Conv1d(2048, 2048, kernel_size=(4,), stride=(1,), padding=(3,), groups=2048)\n",
       "          )\n",
       "          (conv_act_fn): SiLU()\n",
       "          (mlstm_cell): mLSTMCell(\n",
       "            (igate): Linear(in_features=6144, out_features=4, bias=True)\n",
       "            (fgate): Linear(in_features=6144, out_features=4, bias=True)\n",
       "            (outnorm): MultiHeadLayerNorm()\n",
       "          )\n",
       "          (ogate_act_fn): SiLU()\n",
       "          (proj_down): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_blocks_norm): LayerNorm()\n",
       "  )\n",
       "  (token_embedding): Embedding(50304, 1024)\n",
       "  (emb_dropout): Identity()\n",
       "  (lm_head): Linear(in_features=1024, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = xlstm_stack.to(\"cuda\")\n",
    "model.lm_head = torch.nn.Linear(1024, 2)\n",
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = xlstm_stack(x)\n",
    "y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-09-19T17:10:28.728787Z",
     "iopub.status.busy": "2024-09-19T17:10:28.728385Z",
     "iopub.status.idle": "2024-09-20T03:37:05.179864Z",
     "shell.execute_reply": "2024-09-20T03:37:05.178362Z",
     "shell.execute_reply.started": "2024-09-19T17:10:28.728728Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtruonggiabjnh2003\u001b[0m (\u001b[33mtruonggiabjnh2003-fpt-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/HardDisk/binh230_intern/Mamba-AI-generated-text-detection/train/finetune_model/wandb/run-20241110_143617-vi0w5d09</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/truonggiabjnh2003-fpt-university/Detect%20AI%20Generated%20Text/runs/vi0w5d09' target=\"_blank\">xLSTM-Base-Run-1</a></strong> to <a href='https://wandb.ai/truonggiabjnh2003-fpt-university/Detect%20AI%20Generated%20Text' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/truonggiabjnh2003-fpt-university/Detect%20AI%20Generated%20Text' target=\"_blank\">https://wandb.ai/truonggiabjnh2003-fpt-university/Detect%20AI%20Generated%20Text</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/truonggiabjnh2003-fpt-university/Detect%20AI%20Generated%20Text/runs/vi0w5d09' target=\"_blank\">https://wandb.ai/truonggiabjnh2003-fpt-university/Detect%20AI%20Generated%20Text/runs/vi0w5d09</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10361 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      " 19%|█▉        | 1999/10361 [46:10<3:12:53,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2000/10361: Avg Loss = 0.5369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.5160, Accuracy: 76.95%, AuROC: 49.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 2000/10361 [46:53<32:12:18, 13.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved with AUROC: 0.4992471505948029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▊      | 3999/10361 [1:32:56<2:26:08,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4000/10361: Avg Loss = 0.4643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.4331, Accuracy: 81.89%, AuROC: 73.79%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▊      | 4000/10361 [1:33:39<24:28:36, 13.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved with AUROC: 0.7379326028444209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 5999/10361 [2:19:44<1:40:24,  1.38s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6000/10361: Avg Loss = 0.4018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      " 58%|█████▊    | 6000/10361 [2:20:26<16:30:34, 13.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3512, Accuracy: 84.40%, AuROC: 68.13%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 7999/10361 [3:06:30<54:23,  1.38s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 8000/10361: Avg Loss = 0.3462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2990, Accuracy: 87.91%, AuROC: 83.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 8000/10361 [3:07:13<9:08:00, 13.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved with AUROC: 0.8323575712766665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 9999/10361 [4:27:31<17:31,  2.90s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10000/10361: Avg Loss = 0.3190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2694, Accuracy: 90.29%, AuROC: 83.39%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 10000/10361 [4:29:03<2:57:59, 29.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved with AUROC: 0.8338632700870605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10361/10361 [4:45:11<00:00,  1.16s/it]  TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 10361/10361 [4:45:11<00:00,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Initialize WandB\n",
    "wandb.init(project=\"Detect AI Generated Text\", \n",
    "           name=\"xLSTM-Base-Run-1\",\n",
    "           config={\n",
    "               \"learning_rate\": 1e-3,\n",
    "               \"label_smoothing\": 0.03,\n",
    "               \"batch_size\": 16,\n",
    "               \"num_epochs\": 1,\n",
    "               \"optimizer\": \"AdamW\",\n",
    "               \"model\": \"xLSTM\",\n",
    "               \"model_params\": sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "           })\n",
    "\n",
    "# Training configuration\n",
    "label_smoothing = 0.03\n",
    "max_learning_rate = 4e-5\n",
    "print_steps = 500\n",
    "row_threshold = 50000\n",
    "output_subdir = '/home/HardDisk/binh230_intern/Mamba-AI-generated-text-detection/train/finetune_model/results/weight_model'\n",
    "\n",
    "# Send model to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# DataLoader Setup\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4, pin_memory=True, collate_fn=data_collator)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4, pin_memory=True, collate_fn=data_collator)\n",
    "\n",
    "# Optimizer, Criterion, and Scaler\n",
    "optimizer = AdamW(model.parameters(), lr=max_learning_rate, weight_decay=0.1)\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing).to(device)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# Scheduler\n",
    "total_steps = len(train_data_loader)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=max_learning_rate, total_steps=total_steps, pct_start=0.1, anneal_strategy='linear')\n",
    "\n",
    "# Track best model\n",
    "best_auroc = float('-inf')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Compute metrics for model evaluation, including AUROC.\n",
    "    Args:\n",
    "        eval_pred: tuple of (predictions, labels) where predictions are logits.\n",
    "    Returns:\n",
    "        Dictionary containing the computed metrics.\n",
    "    \"\"\"\n",
    "    # Unpack predictions and labels\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits# Get the predicted class\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "\n",
    "    # Calculate precision, recall, and F1-score\n",
    "    precision = precision_score(labels, preds, average='weighted')\n",
    "    recall = recall_score(labels, preds, average='weighted')\n",
    "    f1 = f1_score(labels, preds, average='weighted')\n",
    "\n",
    "    # For binary classification, take the probability of the positive class (class 1)\n",
    "    # Calculate AUROC\n",
    "    # If predictions are logits, convert to probabilities\n",
    "    if len(preds.shape) > 1:  # Check if predictions are 2D (contains probabilities/logits for each class)\n",
    "        probas = torch.softmax(preds, axis=1) if isinstance(preds, np.ndarray) else torch.softmax(preds, dim=1).numpy()\n",
    "        # For binary classification\n",
    "        if probas.shape[1] == 2:\n",
    "            auroc = roc_auc_score(labels, probas[:, 1])\n",
    "        else:  # For multi-class, use macro averaging\n",
    "            auroc = roc_auc_score(labels, probas, multi_class='ovr', average='macro')\n",
    "    else:  # If predictions are already probabilities\n",
    "        auroc = roc_auc_score(labels, preds)\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'auroc': auroc\n",
    "    }\n",
    "\n",
    "def evaluate_model(val_loader):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_labels, all_preds = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs = batch['input_ids'].to(device)\n",
    "            masks = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(inputs)\n",
    "                # Reshape logits and labels\n",
    "                logits = outputs.mean(dim=1)\n",
    "                labels = labels\n",
    "\n",
    "                loss = F.cross_entropy(logits, labels, label_smoothing=label_smoothing)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(torch.argmax(logits, dim=-1).cpu().numpy())  # Collect predicted classes as numpy arrays\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    metrics = compute_metrics((torch.tensor(all_preds), torch.tensor(all_labels)))  # Convert lists to tensors for metrics calculation\n",
    "\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}, Accuracy: {metrics['accuracy']*100:.2f}%, AuROC: {metrics['auroc']*100:.2f}%\")\n",
    "    wandb.log({\"val_loss\": avg_val_loss, **metrics})\n",
    "    \n",
    "    return avg_val_loss, metrics\n",
    "\n",
    "# Set accumulation steps for gradient accumulation\n",
    "accumulation_steps = 4  # Update this based on available memory and desired batch size\n",
    "\n",
    "def train_one_epoch(epoch, train_loader, val_loader):\n",
    "    total_loss = 0.0\n",
    "    all_labels, all_preds = [], []\n",
    "    for batch_index, batch in enumerate(tqdm(train_loader)):\n",
    "        inputs = batch['input_ids'].to(device)\n",
    "        masks = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        model.train()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(inputs)\n",
    "            logits = outputs.mean(dim=1)\n",
    "            loss = F.cross_entropy(logits, labels, label_smoothing=label_smoothing) / accumulation_steps  # Scale loss\n",
    "        \n",
    "        scaler.scale(loss).backward()  # Accumulate gradients\n",
    "        total_loss += loss.item() * accumulation_steps  # Scale back to normal for logging\n",
    "        \n",
    "        # Step only after `accumulation_steps` steps\n",
    "        if (batch_index + 1) % accumulation_steps == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Collect labels and predictions for metrics\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(torch.argmax(logits, dim=-1).cpu().numpy())\n",
    "\n",
    "        # Logging at intervals\n",
    "        if (batch_index + 1) % (print_steps * accumulation_steps) == 0:\n",
    "            avg_loss = total_loss / (print_steps * accumulation_steps)\n",
    "            print(f\"Step {batch_index + 1}/{total_steps}: Avg Loss = {avg_loss:.4f}\")\n",
    "            wandb.log({\"train_loss\": avg_loss, \"learning_rate\": scheduler.get_last_lr()[0]})\n",
    "            total_loss = 0.0\n",
    "\n",
    "            # Evaluate the model and save if AUROC improves\n",
    "            val_loss, metrics = evaluate_model(val_loader)\n",
    "            global best_auroc\n",
    "            if metrics['auroc'] > best_auroc:\n",
    "                best_auroc = metrics['auroc']\n",
    "                torch.save(model.state_dict(), f\"{output_subdir}/xLST-base_step_{batch_index+1}_auroc_{metrics['auroc']:.4f}.pth\")\n",
    "                print(\"Best model saved with AUROC:\", best_auroc)\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(1):  # Define `epochs` outside this loop if more epochs are needed\n",
    "    print(f\"Epoch {epoch+1}/1\")\n",
    "    train_one_epoch(epoch, train_data_loader, test_data_loader)\n",
    "\n",
    "# wandb.finish()\n",
    "print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b><span style='color:#F1A424'>Confusion Matrix</span></b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-20T03:37:05.185888Z",
     "iopub.status.idle": "2024-09-20T03:37:05.186360Z",
     "shell.execute_reply": "2024-09-20T03:37:05.186134Z",
     "shell.execute_reply.started": "2024-09-20T03:37:05.186109Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'oof_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Assuming df is your pandas DataFrame\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m oof_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43moof_df\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreds\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: binarize(x, \u001b[38;5;241m0.5\u001b[39m))\n\u001b[1;32m     16\u001b[0m true_labels \u001b[38;5;241m=\u001b[39m oof_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m     17\u001b[0m predicted_labels \u001b[38;5;241m=\u001b[39m oof_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n",
      "\u001b[0;31mNameError\u001b[0m: name 'oof_df' is not defined"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def binarize(x, threshold):\n",
    "    if x > threshold:\n",
    "        x = 1\n",
    "    else:\n",
    "        x = 0\n",
    "    return x\n",
    "\n",
    "# Assuming df is your pandas DataFrame\n",
    "oof_df[\"binary\"] = oof_df[\"preds\"].apply(lambda x: binarize(x, 0.5))\n",
    "true_labels = oof_df[\"generated\"].values\n",
    "predicted_labels = oof_df[\"binary\"].values\n",
    "\n",
    "# Get the unique classes from both true and predicted labels\n",
    "classes = np.unique(np.concatenate((true_labels, predicted_labels)))\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(true_labels, predicted_labels, labels=classes)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=classes, yticklabels=classes)\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.title(\"Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 7516023,
     "sourceId": 61542,
     "sourceType": "competition"
    },
    {
     "datasetId": 3936750,
     "sourceId": 6847931,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4325258,
     "sourceId": 7432540,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4336615,
     "sourceId": 7452416,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30762,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "binh_mamba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
