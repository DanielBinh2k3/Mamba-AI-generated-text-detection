{"cells":[{"cell_type":"markdown","metadata":{},"source":["# <b><span style='color:#F1A424'>|</span> Import Libraries</b><a class='anchor' id='import_libraries'></a> [↑](#top) \n","\n","***\n","\n","Import all the required libraries for this notebook."]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-09-19T17:05:23.520178Z","iopub.status.busy":"2024-09-19T17:05:23.519758Z","iopub.status.idle":"2024-09-19T17:05:27.482622Z","shell.execute_reply":"2024-09-19T17:05:27.481379Z","shell.execute_reply.started":"2024-09-19T17:05:23.520134Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Current device is: cuda\n","mkdir: cannot create directory ‘output’: File exists\n"]}],"source":["import matplotlib.pyplot as plt\n","import pandas as pd\n","\n","import warnings\n","# import wandb\n","\n","\n","from sklearn.metrics import roc_auc_score\n","from sklearn.utils import shuffle\n","from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n","import torch\n","import torch.nn as nn\n","from torch.nn import Parameter\n","import torch.nn.functional as F\n","from torch.optim import Adam, SGD, AdamW\n","from torch.optim.lr_scheduler import OneCycleLR\n","from torch.utils.data import DataLoader, Dataset\n","from tqdm.auto import tqdm\n","\n","# ======= OPTIONS =========\n","pd.set_option('display.max_rows', 500)\n","pd.set_option('display.max_columns', 500)\n","pd.set_option('display.width', 1000)\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Current device is: {device}\")\n","warnings.filterwarnings(\"ignore\")\n","!mkdir output"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-09-19T17:05:27.484828Z","iopub.status.busy":"2024-09-19T17:05:27.484129Z","iopub.status.idle":"2024-09-19T17:05:43.959428Z","shell.execute_reply":"2024-09-19T17:05:43.958637Z","shell.execute_reply.started":"2024-09-19T17:05:27.484792Z"},"trusted":true},"outputs":[],"source":["import random\n","import torch.nn as nn\n","from torch.nn import BCEWithLogitsLoss\n","from collections import namedtuple\n","from dataclasses import dataclass, field, asdict\n","from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel\n","from mamba_ssm.utils.hf import load_config_hf, load_state_dict_hf\n","# from huggingface_hub import HfApi\n","\n","# import evaluate\n","import numpy as np\n","# from datasets import load_dataset\n","# from transformers import Trainer\n","from transformers import DataCollatorWithPadding\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","import re"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2024-09-18T04:07:22.734063Z","iopub.status.busy":"2024-09-18T04:07:22.733462Z","iopub.status.idle":"2024-09-18T04:07:22.850537Z","shell.execute_reply":"2024-09-18T04:07:22.849644Z","shell.execute_reply.started":"2024-09-18T04:07:22.734029Z"},"trusted":true},"source":["import wandb\n","from huggingface_hub import login\n","\n","login(token=\"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\")\n","notes = \"Train Mamba With 400k row dataset\""]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.012589,"end_time":"2022-08-31T07:03:04.13341","exception":false,"start_time":"2022-08-31T07:03:04.120821","status":"completed"},"tags":[]},"source":["# <b><span style='color:#F1A424'>|</span> Load Data</b><a class='anchor' id='load_data'></a> [↑](#top) \n","\n","***\n","\n","Load data."]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-09-19T17:05:43.962860Z","iopub.status.busy":"2024-09-19T17:05:43.962010Z","iopub.status.idle":"2024-09-19T17:05:53.355460Z","shell.execute_reply":"2024-09-19T17:05:53.354541Z","shell.execute_reply.started":"2024-09-19T17:05:43.962813Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Processing Text: 100%|██████████| 165767/165767 [00:05<00:00, 28178.74it/s]\n","Processing Text: 100%|██████████| 1679/1679 [00:00<00:00, 30375.81it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Trainging DF Processing\n","<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 165767 entries, 0 to 165766\n","Data columns (total 4 columns):\n"," #   Column     Non-Null Count   Dtype \n","---  ------     --------------   ----- \n"," 0   id         165767 non-null  object\n"," 1   prompt_id  165767 non-null  int64 \n"," 2   text       165767 non-null  object\n"," 3   generated  165767 non-null  int64 \n","dtypes: int64(2), object(2)\n","memory usage: 5.1+ MB\n","None\n","Testing DF Processing\n","<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 1679 entries, 0 to 1678\n","Data columns (total 4 columns):\n"," #   Column     Non-Null Count  Dtype \n","---  ------     --------------  ----- \n"," 0   id         1679 non-null   object\n"," 1   prompt_id  1679 non-null   int64 \n"," 2   text       1679 non-null   object\n"," 3   generated  1679 non-null   int64 \n","dtypes: int64(2), object(2)\n","memory usage: 52.6+ KB\n","None\n"]}],"source":["import pandas as pd\n","import re\n","import unicodedata\n","from tqdm import tqdm\n","\n","# Load DataFrame\n","train_df = pd.read_parquet('./data/train_essays.parquet')\n","valid_df = pd.read_parquet('./data/valid_essays.parquet')\n","\n","# Define characters to remove\n","char_to_remove = ['{', '£', '\\x97', '¹', 'å', '\\\\', '\\x85', '<', '\\x99', \n","                  'é', ']', '+', 'Ö', '\\xa0', '>', '|', '\\x80', '~', '©', \n","                  '/', '\\x93', '$', 'Ó', '²', '^', ';', '`', 'á', '*', '(', \n","                  '¶', '®', '[', '\\x94', '\\x91', '#', '-', 'ó', ')', '}', '=']\n","\n","# Define preprocessing function\n","def preprocess_text(text, strategy='light'):    \n","    if strategy == \"none\":\n","        text = text\n","    elif strategy == \"light\":\n","        text = text.encode(\"ascii\", \"ignore\").decode('ascii')        \n","        text = text.strip()\n","        text = text.strip(\"\\\"\")\n","        for c in char_to_remove:\n","            text = text.replace(c, \"\")\n","        if text and text[-1] != \".\":\n","            text = text.split(\".\")\n","            text = \".\".join(text[:-1])\n","            text += \".\"\n","    else:\n","        text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('ascii')\n","        text = text.lower()\n","        text = re.sub(r'[^a-z0-9\\s.,;?!:()\\'\\\"%-]', '', text)\n","        text = re.sub(r'\\s+', ' ', text).strip()\n","    \n","    return text\n","\n","# Apply preprocessing with progress bar\n","tqdm.pandas(desc=\"Processing Text\")\n","train_df['text'] = train_df['text'].progress_apply(lambda x: preprocess_text(x, strategy='light'))\n","valid_df['text'] = valid_df['text'].progress_apply(lambda x: preprocess_text(x, strategy='light'))\n","\n","# Display the first few rows to verify\n","print(\"Trainging DF Processing\")\n","print(train_df.info())\n","print(\"Testing DF Processing\")\n","print(valid_df.info())\n","\n"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.008127,"end_time":"2022-08-31T07:03:11.985369","exception":false,"start_time":"2022-08-31T07:03:11.977242","status":"completed"},"tags":[]},"source":["# <b><span style='color:#F1A424'>|</span> Dataset</b><a class='anchor' id='dataset'></a> [↑](#top) \n","\n","***\n","\n","    \n","We need to get the `max_len` from our `tokenizer`. We create a `tqdm` iterator and for each text we extract the tokenized length. Then we get the maximum value and we add 3 for the special tokens `CLS`, `SEP`, `SEP`.\n","\n","- [Hugging Face Padding and Truncation](https://huggingface.co/docs/transformers/pad_truncation): check truncation to `max_length` or `True` (batch max length)."]},{"cell_type":"markdown","metadata":{},"source":["One sample from the dataset should look as following:\n","```python\n","{\n","\t'inputs': {\n","\t\t'input_ids': tensor([1, 279, 883, ..., 0, 0]),\n","\t\t'token_type_ids': tensor([0, 0, 0, ..., 0, 0]),\n","\t\t'attention_mask': tensor([1, 1, 1, ..., 0, 0])\n","\t},\n","\t'label': tensor([0.0]),\n","\t'ids': '000e8c3c7ddb'\n","}\n","```\n","You can check it by running the cell below."]},{"cell_type":"markdown","metadata":{},"source":["import wandb\n","# Định nghĩa tên project để log thông tin quá trình huấn luyện trên wandb\n","os.environ[\"WANDB_PROJECT\"] = \"mamba_LLM_detect_binary_classification\"\n","os.environ[\"WANDB_API_KEY \"] = \"e7432690ce6d9bfdee410567f89d7e38844ed584\"\n","\n","\n","wandb.login()\n","# start a new wandb run to track this script\n","wandb.init(\n","    # set the wandb project where this run will be logged\n","    project=\"mamba_LLM_detect_binary_classification\",\n","\n","    # track hyperparameters and run metadata\n","    config={\n","    \"learning_rate\": 6e-5,\n","    \"architecture\": \"Mamba-130m-with-Linear-Head\",\n","    \"dataset\": \"Test\",\n","    \"epochs\": 1,\n","    \"lr_scheduler_type\": \"cosine\"\n","    }\n",")"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.008073,"end_time":"2022-08-31T07:03:17.933189","exception":false,"start_time":"2022-08-31T07:03:17.925116","status":"completed"},"tags":[]},"source":["# <b><span style='color:#F1A424'>|</span> Model</b><a class='anchor' id='model'></a> [↑](#top) \n","\n","***"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-09-19T17:05:53.357180Z","iopub.status.busy":"2024-09-19T17:05:53.356739Z","iopub.status.idle":"2024-09-19T17:05:53.373217Z","shell.execute_reply":"2024-09-19T17:05:53.372238Z","shell.execute_reply.started":"2024-09-19T17:05:53.357130Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>prompt_id</th>\n","      <th>text</th>\n","      <th>generated</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>e_ddxvqx5i</td>\n","      <td>0</td>\n","      <td>In recent years, there has been a growing move...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>e_hi0yzrcv</td>\n","      <td>0</td>\n","      <td>\\nWhy not cars in our life\\n\\nI have ever met ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>e_uesv4xha</td>\n","      <td>0</td>\n","      <td>A car is considered by many a nessecity for ev...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>e_2tl5ylwy</td>\n","      <td>0</td>\n","      <td>H\\n\\nello fellow citezens , we are here to inf...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>e_s6ci4vj0</td>\n","      <td>0</td>\n","      <td>Have you ever known how if feels not being abl...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["           id  prompt_id                                               text  generated\n","0  e_ddxvqx5i          0  In recent years, there has been a growing move...          1\n","1  e_hi0yzrcv          0  \\nWhy not cars in our life\\n\\nI have ever met ...          1\n","2  e_uesv4xha          0  A car is considered by many a nessecity for ev...          1\n","3  e_2tl5ylwy          0  H\\n\\nello fellow citezens , we are here to inf...          0\n","4  e_s6ci4vj0          0  Have you ever known how if feels not being abl...          1"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["train_df.head()"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-09-19T17:05:53.374547Z","iopub.status.busy":"2024-09-19T17:05:53.374259Z","iopub.status.idle":"2024-09-19T17:05:56.661369Z","shell.execute_reply":"2024-09-19T17:05:56.660369Z","shell.execute_reply.started":"2024-09-19T17:05:53.374516Z"},"trusted":true},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['id', 'prompt_id', 'text', 'labels'],\n","        num_rows: 165767\n","    })\n","    test: Dataset({\n","        features: ['id', 'prompt_id', 'text', 'labels'],\n","        num_rows: 1679\n","    })\n","})"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","from datasets import Dataset, DatasetDict\n","from sklearn.model_selection import train_test_split\n","\n","# Assuming train_df is your DataFrame with a 'text' column\n","# Convert the 'id' column to a string to avoid ArrowTypeError\n","# df['id'] = df['id'].astype(str)\n","\n","# Rename the 'generated' column to 'labels'\n","train_df.rename(columns={'generated': 'labels'}, inplace=True)\n","valid_df.rename(columns={'generated': 'labels'}, inplace=True)\n","\n","# # Access the train and test datasets\n","# train_dataset, test_dataset = train_test_split(df, test_size=0.05)\n","\n","# Combine the splits into a DatasetDict\n","dataset_dict = DatasetDict({\n","    'train': Dataset.from_pandas(train_df),\n","    'test': Dataset.from_pandas(valid_df),\n","})\n","\n","# Display the first example from each dataset\n","dataset_dict"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-09-19T17:05:56.662755Z","iopub.status.busy":"2024-09-19T17:05:56.662437Z","iopub.status.idle":"2024-09-19T17:08:50.995345Z","shell.execute_reply":"2024-09-19T17:08:50.994344Z","shell.execute_reply.started":"2024-09-19T17:05:56.662708Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Map: 100%|██████████| 165767/165767 [01:32<00:00, 1792.16 examples/s]\n","Map: 100%|██████████| 1679/1679 [00:00<00:00, 1976.39 examples/s]\n"]}],"source":["tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neox-20b')\n","# Add eos tokens\n","# tokenizer.eos_token = \"<|endoftext|>\"\n","tokenizer.pad_token = tokenizer.eos_token\n","def preprocess_function(examples):\n","    # Tokenize the text with truncation\n","    samples = tokenizer(examples['text'], \n","                        truncation=True, \n","                        padding='max_length', \n","                        max_length=512,         \n","                        return_tensors=\"pt\")\n","    \n","    return samples\n","\n","# Apply preprocessing to the dataset\n","tokenized_dataset = dataset_dict.map(preprocess_function, batched=True)\n"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/plain":["Zamba2ForSequenceClassification(\n","  (model): Zamba2Model(\n","    (embed_tokens): Embedding(50277, 2048, padding_idx=0)\n","    (blocks): ModuleList(\n","      (0): Zamba2AttentionDecoderLayer(\n","        (self_attn): Zamba2SdpaAttention(\n","          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n","          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n","          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n","          (o_proj): Linear(in_features=4096, out_features=2048, bias=False)\n","          (linear_q_lora_A_list): ParameterList(\n","              (0): Object of type: Linear\n","              (1): Object of type: Linear\n","            (0): Linear(in_features=4096, out_features=128, bias=False)\n","            (1): Linear(in_features=4096, out_features=128, bias=False)\n","          )\n","          (linear_q_lora_B_list): ParameterList(\n","              (0): Object of type: Linear\n","              (1): Object of type: Linear\n","            (0): Linear(in_features=128, out_features=4096, bias=False)\n","            (1): Linear(in_features=128, out_features=4096, bias=False)\n","          )\n","          (linear_k_lora_A_list): ParameterList(\n","              (0): Object of type: Linear\n","              (1): Object of type: Linear\n","            (0): Linear(in_features=4096, out_features=128, bias=False)\n","            (1): Linear(in_features=4096, out_features=128, bias=False)\n","          )\n","          (linear_k_lora_B_list): ParameterList(\n","              (0): Object of type: Linear\n","              (1): Object of type: Linear\n","            (0): Linear(in_features=128, out_features=4096, bias=False)\n","            (1): Linear(in_features=128, out_features=4096, bias=False)\n","          )\n","          (linear_v_lora_A_list): ParameterList(\n","              (0): Object of type: Linear\n","              (1): Object of type: Linear\n","            (0): Linear(in_features=4096, out_features=128, bias=False)\n","            (1): Linear(in_features=4096, out_features=128, bias=False)\n","          )\n","          (linear_v_lora_B_list): ParameterList(\n","              (0): Object of type: Linear\n","              (1): Object of type: Linear\n","            (0): Linear(in_features=128, out_features=4096, bias=False)\n","            (1): Linear(in_features=128, out_features=4096, bias=False)\n","          )\n","          (rotary_emb): Zamba2RotaryEmbedding()\n","        )\n","        (feed_forward): Zamba2MLP(\n","          (linear_fc1): Linear(in_features=2048, out_features=16384, bias=False)\n","          (linear_fc2): Linear(in_features=8192, out_features=2048, bias=False)\n","          (linear_fc1_lora_A_list): ParameterList(\n","              (0): Object of type: Linear\n","              (1): Object of type: Linear\n","            (0): Linear(in_features=2048, out_features=128, bias=False)\n","            (1): Linear(in_features=2048, out_features=128, bias=False)\n","          )\n","          (linear_fc1_lora_B_list): ParameterList(\n","              (0): Object of type: Linear\n","              (1): Object of type: Linear\n","            (0): Linear(in_features=128, out_features=16384, bias=False)\n","            (1): Linear(in_features=128, out_features=16384, bias=False)\n","          )\n","        )\n","        (input_layernorm): Zamba2RMSNorm()\n","        (pre_ff_layernorm): Zamba2RMSNorm()\n","      )\n","    )\n","    (mamba_layers): ModuleList(\n","      (0-13): 14 x Zamba2MambaDecoderLayer(\n","        (mamba): Mamba2Layer(\n","          (in_proj): Linear(in_features=2048, out_features=8512, bias=False)\n","          (conv1d): Conv1d(4352, 4352, kernel_size=(4,), stride=(1,), padding=(3,), groups=4352)\n","          (act): SiLU()\n","          (norm): RMSNorm()\n","          (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n","        )\n","        (input_layernorm): Zamba2RMSNorm()\n","      )\n","    )\n","    (linear_layers): ModuleList(\n","      (0-1): 2 x Linear(in_features=2048, out_features=2048, bias=False)\n","    )\n","    (final_layernorm): Zamba2RMSNorm()\n","  )\n","  (score): Linear(in_features=2048, out_features=2, bias=False)\n",")"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","import numpy as np\n","from transformers import DataCollatorWithPadding\n","\n","# Dataset and Tokenizer Setup\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","\n","from transformers import Zamba2Config, Zamba2ForCausalLM, Zamba2ForSequenceClassification,Zamba2Model\n","\n","model_config = Zamba2Config.from_pretrained(\"/home/HardDisk/binh230_intern/zamba_test/config.json\")\n","# model_config.num_labels = 10\n","model = Zamba2ForSequenceClassification(model_config)\n","model.to(\"cuda\")\n"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-09-19T17:08:50.996892Z","iopub.status.busy":"2024-09-19T17:08:50.996548Z","iopub.status.idle":"2024-09-19T17:08:51.011804Z","shell.execute_reply":"2024-09-19T17:08:51.010782Z","shell.execute_reply.started":"2024-09-19T17:08:50.996858Z"},"trusted":true},"outputs":[],"source":["# Set seed cho hàm random\n","random.seed(42)\n","\n","# Tạo tập train và test\n","train_dataset = tokenized_dataset[\"train\"]\n","test_dataset = tokenized_dataset[\"test\"]\n","#  Drop the 'prompt_id' feature from both datasets\n","train_dataset = train_dataset.remove_columns([\"text\"]).remove_columns([\"id\"])\n","test_dataset = test_dataset.remove_columns([\"text\"]).remove_columns([\"id\"])\n","\n","# Tạo tập evaluation để đánh giá trong lúc train\n","# Do số lượng tập test lớn nên chỉ lấy mẫu 1% tập dữ liệu test để đánh giá\n","# total_samples = len(test_dataset)\n","# eval_samples = int(0.5 * total_samples)\n","# eval_indices = random.sample(range(total_samples), eval_samples)\n","# eval_dataset = test_dataset.select(eval_indices)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-09-19T17:10:28.716075Z","iopub.status.busy":"2024-09-19T17:10:28.715683Z","iopub.status.idle":"2024-09-19T17:10:28.726787Z","shell.execute_reply":"2024-09-19T17:10:28.726000Z","shell.execute_reply.started":"2024-09-19T17:10:28.716039Z"},"trusted":true},"outputs":[],"source":["import torch\n","import numpy as np\n","from tqdm import tqdm\n","from sklearn.metrics import roc_auc_score, accuracy_score\n","\n","def TestModel(test_data_loader, model, criterion):\n","    test_losses = []\n","    all_predictions = []\n","    all_actual_values = []\n","    \n","    with torch.no_grad():\n","        model.eval()  # Set model to evaluation mode\n","        for batch in tqdm(test_data_loader):\n","            if len(batch.input_ids) == 0:\n","                # Safeguard against empty sequences.\n","                continue\n","\n","            # Move tensors to GPU if available\n","            token_sequences = batch.input_ids.cuda()\n","            attention_masks = batch.attention_mask.cuda()\n","            labels = batch.labels.cuda()\n","            \n","            # Forward pass\n","            output = model(input_ids=token_sequences, attention_mask=attention_masks, labels=labels)\n","            logits = output.logits\n","            loss = output.loss\n","\n","            test_losses.append(loss.item())  # Convert loss to scalar before appending\n","\n","            # Apply softmax to logits to get predicted probabilities\n","            probabilities = logits.softmax(dim=1)[:, 1]\n","            all_predictions.extend(probabilities.cpu().numpy())\n","            all_actual_values.extend(labels.cpu().numpy())\n","\n","    all_predictions, all_actual_values = np.array(all_predictions), np.array(all_actual_values)\n","\n","    # Compute AUROC\n","    auroc = roc_auc_score(all_actual_values, all_predictions)\n","    \n","    # Binarize predictions and compute accuracy\n","    binary_predictions = (all_predictions > 0.6).astype(int)\n","    accuracy = accuracy_score(all_actual_values, binary_predictions)\n","    \n","    return accuracy, auroc, np.mean(test_losses)\n"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"text/plain":["Dataset({\n","    features: ['prompt_id', 'labels', 'input_ids', 'attention_mask'],\n","    num_rows: 165767\n","})"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["train_dataset"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"data":{"text/html":["Finishing last run (ID:pr1uxfpt) before initializing another..."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">major-durian-1</strong> at: <a href='https://wandb.ai/truonggiabjnh2003-fpt-university/ZambaModel/runs/pr1uxfpt' target=\"_blank\">https://wandb.ai/truonggiabjnh2003-fpt-university/ZambaModel/runs/pr1uxfpt</a><br/> View project at: <a href='https://wandb.ai/truonggiabjnh2003-fpt-university/ZambaModel' target=\"_blank\">https://wandb.ai/truonggiabjnh2003-fpt-university/ZambaModel</a><br/>Synced 4 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20241023_001310-pr1uxfpt/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Successfully finished last run (ID:pr1uxfpt). Initializing new run:<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.18.5"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/home/HardDisk/binh230_intern/Mamba-AI-generated-text-detection/wandb/run-20241023_001518-6fddiv68</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/truonggiabjnh2003-fpt-university/Detect%20AI%20Generated%20Text/runs/6fddiv68' target=\"_blank\">sunny-dream-1</a></strong> to <a href='https://wandb.ai/truonggiabjnh2003-fpt-university/Detect%20AI%20Generated%20Text' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/truonggiabjnh2003-fpt-university/Detect%20AI%20Generated%20Text' target=\"_blank\">https://wandb.ai/truonggiabjnh2003-fpt-university/Detect%20AI%20Generated%20Text</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/truonggiabjnh2003-fpt-university/Detect%20AI%20Generated%20Text/runs/6fddiv68' target=\"_blank\">https://wandb.ai/truonggiabjnh2003-fpt-university/Detect%20AI%20Generated%20Text/runs/6fddiv68</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["lr = 4e-05, label_smoothing = 0.03, output_subdir = 3090_1\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/20721 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","  0%|          | 38/20721 [01:24<12:42:24,  2.21s/it]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[15], line 112\u001b[0m\n\u001b[1;32m    110\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()  \n\u001b[1;32m    111\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m1\u001b[39m)  \n\u001b[0;32m--> 112\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m    113\u001b[0m lr_schedule\u001b[38;5;241m.\u001b[39mstep()  \n\u001b[1;32m    115\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu())\n","File \u001b[0;32m~/anaconda3/envs/binh_mamba/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:130\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m opt \u001b[38;5;241m=\u001b[39m opt_ref()\n\u001b[1;32m    129\u001b[0m opt\u001b[38;5;241m.\u001b[39m_opt_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__get__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/anaconda3/envs/binh_mamba/lib/python3.12/site-packages/torch/optim/optimizer.py:484\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    480\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    481\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    482\u001b[0m             )\n\u001b[0;32m--> 484\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    487\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/envs/binh_mamba/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/home/HardDisk/binh230_intern/transformers_zamba2/src/transformers/optimization.py:656\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    653\u001b[0m     bias_correction2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m beta2 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    654\u001b[0m     step_size \u001b[38;5;241m=\u001b[39m step_size \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(bias_correction2) \u001b[38;5;241m/\u001b[39m bias_correction1\n\u001b[0;32m--> 656\u001b[0m \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcdiv_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_avg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mstep_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;66;03m# Just adding the square of the weights to the loss function is *not*\u001b[39;00m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;66;03m# the correct way of using L2 regularization/weight decay with Adam,\u001b[39;00m\n\u001b[1;32m    660\u001b[0m \u001b[38;5;66;03m# since that will interact with the m and v parameters in strange ways.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;66;03m# of the weights to the loss with plain (non-momentum) SGD.\u001b[39;00m\n\u001b[1;32m    665\u001b[0m \u001b[38;5;66;03m# Add weight decay at the end (fixed version)\u001b[39;00m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","from tqdm import tqdm\n","from transformers import AdamW, DataCollatorWithPadding\n","import wandb  # Add wandb\n","\n","# Set random seed for reproducibility\n","def set_seed(seed):\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","# Set a specific seed value\n","set_seed(100)\n","\n","# Accuracy Calculation\n","def compute_accuracy(predictions, labels):\n","    preds = torch.argmax(predictions, dim=1)\n","    correct = torch.sum(preds == labels)\n","    return correct.item() / len(labels)\n","\n","# Initialize wandb run\n","wandb.init(project=\"Detect AI Generated Text\", \n","           name=\"First run of Zamba\",\n","           config={\n","               \"learning_rate\": 4e-5,\n","               \"label_smoothing\": 0.03,\n","               \"batch_size\": 8,\n","               \"num_epochs\": 1,\n","               \"optimizer\": \"AdamW\",\n","               \"model\": model.config.model_type,\n","               \"model_params\": sum(p.numel() for p in model.parameters() if p.requires_grad)\n","           })\n","\n","config = wandb.config  # Access the configuration\n","\n","# Variables for the experiment\n","label_smoothing = config.label_smoothing\n","output_subdir = '3090_1'\n","max_learning_rate = config.learning_rate\n","\n","# Run experiment\n","for max_learning_rate in [max_learning_rate]:\n","    print(f'lr = {max_learning_rate}, label_smoothing = {label_smoothing}, output_subdir = {output_subdir}')\n","    \n","    # Dataloader Setup\n","    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","    train_data_loader = DataLoader(\n","        train_dataset, \n","        batch_size=config.batch_size,  \n","        num_workers=4, \n","        shuffle=True, \n","        pin_memory=True, \n","        collate_fn=data_collator\n","    )\n","    test_data_loader = DataLoader(\n","        test_dataset, \n","        batch_size=config.batch_size,  \n","        num_workers=4, \n","        shuffle=False, \n","        pin_memory=True, \n","        collate_fn=data_collator\n","    )\n","\n","    # Optimizer and Criterion Setup\n","    optimizer = AdamW(\n","        model.parameters(),\n","        lr=max_learning_rate,\n","        weight_decay=0.1\n","    )\n","    criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n","\n","    total_step_count = len(train_data_loader)\n","    lr_schedule = torch.optim.lr_scheduler.CosineAnnealingLR(\n","        optimizer=optimizer,\n","        T_max=total_step_count,  \n","        eta_min=1e-8  \n","    )\n","\n","    best_auroc = -99999999\n","    train_losses = []\n","    model.train()\n","\n","    total_rows_processed = 0\n","    row_threshold = 50000\n","    print_steps = 500  \n","\n","    for batch_index, train_batch in enumerate(tqdm(train_data_loader)):\n","        if len(train_batch.input_ids) == 0:\n","            continue\n","\n","        token_sequences = train_batch.input_ids.to(\"cuda\")\n","        attention_masks = train_batch.attention_mask.to(\"cuda\")\n","        labels = train_batch.labels.to(\"cuda\")\n","\n","        optimizer.zero_grad()\n","\n","        # Forward pass\n","        output = model(input_ids=token_sequences, attention_mask=attention_masks, labels=labels)\n","        logits = output.logits\n","        loss = output.loss\n","\n","        # Training accuracy\n","        accuracy = compute_accuracy(logits, labels)\n","\n","        loss.backward()  \n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)  \n","        optimizer.step()  \n","        lr_schedule.step()  \n","\n","        train_losses.append(loss.detach().cpu())\n","\n","        if (batch_index + 1) % print_steps == 0:\n","            avg_train_loss = sum(train_losses) / len(train_losses)\n","            print(f\"Step {batch_index+1}/{total_step_count}: Avg Train Loss = {avg_train_loss:.4f}, Train Accuracy = {accuracy*100:.2f}%\")\n","            wandb.log({\"train_loss\": avg_train_loss, \"train_accuracy\": accuracy * 100, \"learning_rate\": lr_schedule.get_last_lr()[0]})\n","            train_losses = []  \n","\n","        total_rows_processed += len(train_batch.input_ids)\n","\n","        if total_rows_processed >= row_threshold:\n","            model.eval()\n","            val_accuracy, val_auroc, test_loss = TestModel(test_data_loader, model, criterion)\n","            model.train()\n","            \n","            print(f'Validation Loss: {test_loss:.4f}, Validation Accuracy: {val_accuracy*100:.2f}%, AuROC :{val_auroc*100:.2f}%')\n","            wandb.log({\"val_loss\": test_loss, \"val_accuracy\": val_accuracy * 100, \"val_auroc\": val_auroc * 100})\n","            \n","            total_rows_processed = 0  \n","\n","            if val_auroc > best_auroc:\n","                best_auroc = val_auroc\n","                torch.save(model.state_dict(), f'./Models/BestModel-Val_Accuracy-{val_accuracy*100}%-AuROC_Score-{val_auroc*100}-Loss-{int(test_loss*1000)}.pth')\n","    \n","    print(f'Best AUROC: {best_auroc}')\n","    wandb.log({\"best_auroc\": best_auroc})\n","\n","wandb.finish()\n"]},{"cell_type":"code","execution_count":10,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2024-09-19T17:10:28.728787Z","iopub.status.busy":"2024-09-19T17:10:28.728385Z","iopub.status.idle":"2024-09-20T03:37:05.179864Z","shell.execute_reply":"2024-09-20T03:37:05.178362Z","shell.execute_reply.started":"2024-09-19T17:10:28.728728Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["lr = 2e-05, label_smoothing = 0.03, output_subdir = 3090_1\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/82884 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","Zamba2 requires an initialized `HybridMambaAttentionDynamicCache` to return a cache. None was provided, so no cache will be returned.\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","  0%|          | 0/82884 [00:49<?, ?it/s]\n"]},{"ename":"RuntimeError","evalue":"\"_amp_foreach_non_finite_check_and_unscale_cuda\" not implemented for 'BFloat16'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[10], line 102\u001b[0m\n\u001b[1;32m     99\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m compute_accuracy(logits, labels)\n\u001b[1;32m    101\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m--> 102\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munscale_\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    104\u001b[0m scaler\u001b[38;5;241m.\u001b[39mstep(optimizer)\n","File \u001b[0;32m~/anaconda3/envs/binh_mamba/lib/python3.12/site-packages/torch/amp/grad_scaler.py:338\u001b[0m, in \u001b[0;36mGradScaler.unscale_\u001b[0;34m(self, optimizer)\u001b[0m\n\u001b[1;32m    335\u001b[0m inv_scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scale\u001b[38;5;241m.\u001b[39mdouble()\u001b[38;5;241m.\u001b[39mreciprocal()\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m    336\u001b[0m found_inf \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfull((), \u001b[38;5;241m0.0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scale\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 338\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_unscale_grads_\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minv_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m    340\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mUNSCALED\n","File \u001b[0;32m~/anaconda3/envs/binh_mamba/lib/python3.12/site-packages/torch/amp/grad_scaler.py:279\u001b[0m, in \u001b[0;36mGradScaler._unscale_grads_\u001b[0;34m(self, optimizer, inv_scale, found_inf, allow_fp16)\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m device, per_dtype_grads \u001b[38;5;129;01min\u001b[39;00m per_device_and_dtype_grads\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m grads \u001b[38;5;129;01min\u001b[39;00m per_dtype_grads\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m--> 279\u001b[0m             \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_amp_foreach_non_finite_check_and_unscale_\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m                \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m                \u001b[49m\u001b[43mper_device_found_inf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m                \u001b[49m\u001b[43mper_device_inv_scale\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m per_device_found_inf\u001b[38;5;241m.\u001b[39m_per_device_tensors\n","\u001b[0;31mRuntimeError\u001b[0m: \"_amp_foreach_non_finite_check_and_unscale_cuda\" not implemented for 'BFloat16'"]}],"source":["import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","from tqdm import tqdm\n","from transformers import Adafactor\n","\n","# Set random seed for reproducibility\n","def set_seed(seed):\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)  # for multi-GPU setups\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","# Set a specific seed value\n","set_seed(100)\n","\n","# Accuracy Calculation\n","def compute_accuracy(predictions, labels):\n","    preds = torch.argmax(predictions, dim=1)\n","    correct = torch.sum(preds == labels)\n","    return correct.item() / len(labels)\n","\n","# Variables for the experiment\n","label_smoothing = 0.03\n","output_subdir = '3090_1'\n","max_learning_rates = [2e-5]\n","\n","# Run experiment\n","for max_learning_rate in max_learning_rates:\n","    print(f'lr = {max_learning_rate}, label_smoothing = {label_smoothing}, output_subdir = {output_subdir}')\n","    \n","    # Dataloader Setup\n","    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","    train_data_loader = DataLoader(\n","        train_dataset, \n","        batch_size=2,  # Increased batch size since it will be split across GPUs\n","        num_workers=4, \n","        shuffle=True, \n","        pin_memory=True, \n","        collate_fn=data_collator\n","    )\n","    test_data_loader = DataLoader(\n","        test_dataset, \n","        batch_size=2,  # Increased batch size\n","        num_workers=4, \n","        shuffle=False, \n","        pin_memory=True, \n","        collate_fn=data_collator\n","    )\n","\n","    # Optimizer, Criterion, and Scaler Setup\n","    optimizer = AdamW(\n","        model.parameters(),\n","        lr=max_learning_rate,\n","        weight_decay=0.1\n","    )\n","    criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n","    scaler = torch.cuda.amp.GradScaler(enabled=True)\n","\n","    total_step_count = len(train_data_loader)\n","    # Cosine Annealing Scheduler\n","    lr_schedule = torch.optim.lr_scheduler.CosineAnnealingLR(\n","        optimizer=optimizer,\n","        T_max=total_step_count,  # The number of steps to reach the minimum learning rate\n","        eta_min=1e-7  # Minimum learning rate (optional, can be adjusted)\n","    )\n","\n","    best_auroc = -99999999\n","    train_losses = []\n","    model.train()\n","\n","    # Tracking the number of rows processed\n","    total_rows_processed = 0\n","    row_threshold = 50000\n","\n","    print_steps = 500  # Log training accuracy/loss every 500 steps\n","\n","    for batch_index, train_batch in enumerate(tqdm(train_data_loader)):\n","        if len(train_batch.input_ids) == 0:\n","            continue\n","\n","        # Send data to GPU(s)\n","        token_sequences = train_batch.input_ids.to(\"cuda\")\n","        attention_masks = train_batch.attention_mask.to(\"cuda\")\n","        labels = train_batch.labels.to(\"cuda\")\n","\n","        optimizer.zero_grad()\n","\n","        with torch.cuda.amp.autocast(enabled=True):\n","            # output = model(token_sequences, attention_masks)\n","            output = model(input_ids=token_sequences, attention_mask=attention_masks, labels=labels)\n","            logits = output.logits\n","            loss = output.loss\n","\n","        # Training accuracy\n","        accuracy = compute_accuracy(logits, labels)\n","\n","        scaler.scale(loss).backward()\n","        scaler.unscale_(optimizer)\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n","        scaler.step(optimizer)\n","        scaler.update()\n","        lr_schedule.step()\n","\n","        train_losses.append(loss.detach().cpu())\n","\n","        # Log training accuracy and loss every 500 steps\n","        if (batch_index + 1) % print_steps == 0:\n","            avg_train_loss = sum(train_losses) / len(train_losses)\n","            print(f\"Step {batch_index+1}/{total_step_count}: Avg Train Loss = {avg_train_loss:.4f}, Train Accuracy = {accuracy*100:.2f}%\")\n","            train_losses = []  # Reset train loss tracking for the next 500 steps\n","\n","        # Increment the number of rows processed\n","        total_rows_processed += len(train_batch.input_ids)\n","\n","        # Evaluate the model every 50,000 rows\n","        if total_rows_processed >= row_threshold:\n","            model.eval()\n","            val_accuracy, val_auroc, test_loss = TestModel(test_data_loader, model, criterion)\n","            model.train()\n","            \n","            print(f'Validation Loss: {test_loss:.4f}, Validation Accuracy: {val_accuracy*100:.2f}%, AuROC :{val_auroc*100:.2f}%')\n","            \n","            total_rows_processed = 0  # Reset after each evaluation\n","\n","            # Save model and reset\n","            torch.save(model.state_dict(), f'./Models/MambaFormer2-Val_Accuracy-{val_accuracy*100}%-AuROC_Score-{val_auroc*100}-Loss-{int(test_loss*1000)}.pth')\n","    print(f'Best AUROC: {best_auroc}')\n"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"data":{"text/plain":["(tensor([[ 1.1261e-01,  8.0811e-02],\n","         [ 3.6621e-04, -7.1289e-01],\n","         [ 6.4697e-02, -9.7314e-01],\n","         [-7.3584e-01, -1.0107e+00]], device='cuda:0', dtype=torch.float16,\n","        grad_fn=<ViewBackward0>),\n"," tensor([0, 1, 1, 1], device='cuda:0'))"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["output.logits.view(-1, 2), labels.view(-1)"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"data":{"text/plain":["(tensor([[ 1.1261e-01,  8.0811e-02],\n","         [ 3.6621e-04, -7.1289e-01],\n","         [ 6.4697e-02, -9.7314e-01],\n","         [-7.3584e-01, -1.0107e+00]], device='cuda:0', dtype=torch.float16,\n","        grad_fn=<IndexBackward0>),\n"," tensor([0, 1, 1, 1], device='cuda:0'))"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["output.logits, labels"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"data":{"text/plain":["0.25"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["compute_accuracy(output.logits.view(-1, 2), labels.view(-1))"]},{"cell_type":"code","execution_count":13,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2024-09-20T04:14:46.771669Z","iopub.status.busy":"2024-09-20T04:14:46.771286Z","iopub.status.idle":"2024-09-20T04:16:40.585821Z","shell.execute_reply":"2024-09-20T04:16:40.584665Z","shell.execute_reply.started":"2024-09-20T04:14:46.771631Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/105 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","100%|██████████| 105/105 [00:15<00:00,  6.58it/s]\n"]},{"ename":"ValueError","evalue":"too many values to unpack (expected 2)","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m----> 2\u001b[0m val_accuracy, test_loss \u001b[38;5;241m=\u001b[39m TestModel(test_data_loader, model, criterion)\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Validation Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_accuracy\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"]}],"source":["model.eval()\n","val_accuracy, test_loss = TestModel(test_data_loader, model, criterion)\n","model.train()\n","\n","print(f'Validation Loss: {test_loss:.4f}, Validation Accuracy: {val_accuracy*100:.2f}%')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-09-20T03:37:05.182217Z","iopub.status.idle":"2024-09-20T03:37:05.182548Z","shell.execute_reply":"2024-09-20T03:37:05.182399Z","shell.execute_reply.started":"2024-09-20T03:37:05.182382Z"},"trusted":true},"outputs":[],"source":["auroc_scores_by_dataset, test_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-09-20T03:37:05.183718Z","iopub.status.idle":"2024-09-20T03:37:05.184069Z","shell.execute_reply":"2024-09-20T03:37:05.183920Z","shell.execute_reply.started":"2024-09-20T03:37:05.183903Z"},"trusted":true},"outputs":[],"source":["import torch\n","import numpy as np\n","from tqdm import tqdm\n","from sklearn.metrics import roc_auc_score\n","\n","model.eval()\n","auroc_scores_by_dataset, test_loss = TestModel(test_data_loader, model, criterion)\n","model.train()\n","\n","# average_auroc = np.average(auroc_scores_by_dataset, weights=[1, 1])\n","# if (average_auroc > best_auroc) or (max(auroc_scores_by_dataset) > 0.993):\n","#     best_auroc = average_auroc\n","#     if output_subdir is not None:\n","#         torch.save(model.state_dict(), f'Models/Mamba/{output_subdir}/S{step_number}_CTX1024.pth')\n","\n","# train_losses = []"]},{"cell_type":"markdown","metadata":{},"source":["### <b><span style='color:#F1A424'>Confusion Matrix</span></b>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-09-20T03:37:05.185888Z","iopub.status.idle":"2024-09-20T03:37:05.186360Z","shell.execute_reply":"2024-09-20T03:37:05.186134Z","shell.execute_reply.started":"2024-09-20T03:37:05.186109Z"},"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import pandas as pd\n","import seaborn as sns\n","\n","from sklearn.metrics import confusion_matrix\n","\n","def binarize(x, threshold):\n","    if x > threshold:\n","        x = 1\n","    else:\n","        x = 0\n","    return x\n","\n","# Assuming df is your pandas DataFrame\n","oof_df[\"binary\"] = oof_df[\"preds\"].apply(lambda x: binarize(x, 0.5))\n","true_labels = oof_df[\"generated\"].values\n","predicted_labels = oof_df[\"binary\"].values\n","\n","# Get the unique classes from both true and predicted labels\n","classes = np.unique(np.concatenate((true_labels, predicted_labels)))\n","\n","# Compute the confusion matrix\n","cm = confusion_matrix(true_labels, predicted_labels, labels=classes)\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=classes, yticklabels=classes)\n","plt.xlabel(\"Predicted Labels\")\n","plt.ylabel(\"True Labels\")\n","plt.title(\"Confusion Matrix\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":7516023,"sourceId":61542,"sourceType":"competition"},{"datasetId":3936750,"sourceId":6847931,"sourceType":"datasetVersion"},{"datasetId":4325258,"sourceId":7432540,"sourceType":"datasetVersion"},{"datasetId":4336615,"sourceId":7452416,"sourceType":"datasetVersion"}],"dockerImageVersionId":30762,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"binh_mamba","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.5"}},"nbformat":4,"nbformat_minor":4}
