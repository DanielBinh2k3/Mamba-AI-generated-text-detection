{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load detectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5DawBKqzpipt",
        "outputId": "c0b27543-6621-4246-8494-970509d8058c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RobertaForSequenceClassification(\n",
              "  (roberta): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 1024)\n",
              "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-23): 24 x RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (classifier): RobertaClassificationHead(\n",
              "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (out_proj): Linear(in_features=1024, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import transformers\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import random\n",
        "device = \"cuda\" # example: cuda:0\n",
        "detector_path_or_id = \"TrustSafeAI/RADAR-Vicuna-7B\"\n",
        "detector = transformers.AutoModelForSequenceClassification.from_pretrained(detector_path_or_id)\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(detector_path_or_id)\n",
        "detector.eval()\n",
        "detector.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Generate AI-text samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading shards: 100%|██████████| 4/4 [03:16<00:00, 49.22s/it]\n",
            "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.09it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Qwen2ForCausalLM(\n",
              "  (model): Qwen2Model(\n",
              "    (embed_tokens): Embedding(152064, 3584)\n",
              "    (layers): ModuleList(\n",
              "      (0-27): 28 x Qwen2DecoderLayer(\n",
              "        (self_attn): Qwen2SdpaAttention(\n",
              "          (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
              "          (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
              "          (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
              "          (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
              "          (rotary_emb): Qwen2RotaryEmbedding()\n",
              "        )\n",
              "        (mlp): Qwen2MLP(\n",
              "          (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
              "          (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
              "          (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
              "        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
              "      )\n",
              "    )\n",
              "    (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# load generators\n",
        "generator_path_or_id=\"Qwen/Qwen2.5-7B-Instruct\" \n",
        "generator = transformers.AutoModelForCausalLM.from_pretrained(generator_path_or_id)\n",
        "generator_tokenizer = transformers.AutoTokenizer.from_pretrained(generator_path_or_id)\n",
        "generator.eval()\n",
        "generator.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use LLMs to generate compeletions for a text-span. The completed text is the AI-texts.\n",
        "generator_tokenizer.pad_token = tokenizer.eos_token\n",
        "generator_tokenizer.padding_side = 'left'\n",
        "generator_tokenizer.truncation_side = 'right'\n",
        "instruction=\"You are helpful assistant to complete given text:\" # you can choose whatever sentences you like \n",
        "Human_texts = [\n",
        "    \"Maj Richard Scott, 40, is accused of driving at speeds of up to 95mph (153km/h) in bad weather before the smash on a B-road in Wiltshire. Gareth Hicks, 24, suffered fatal injuries when the van he was asleep in was hit by Mr Scott's Audi A6. Maj Scott denies a charge of causing death by careless driving. Prosecutor Charles Gabb alleged the defendant, from Green Lane in Shepperton, Surrey, had crossed the carriageway of the 60mph-limit B390 in Shrewton near Amesbury. The weather was \\\"awful\\\" and there was strong wind and rain, he told jurors. He said Mr Scott's car was described as \\\"twitching\\\" and \\\"may have been aquaplaning\\\" before striking the first vehicle; a BMW driven by Craig Reed. Mr Scott's Audi then returned to his side of the road but crossed the carriageway again before colliding\",\n",
        "    \"Solar concentrating technologies such as parabolic dish, trough and Scheffler reflectors can provide process heat for commercial and industrial applications. The first commercial system was the Solar Total Energy Project (STEP) in Shenandoah, Georgia, USA where a field of 114 parabolic dishes provided 50% of the process heating, air conditioning and electrical requirements for a clothing factory. This grid-connected cogeneration system provided 400 kW of electricity plus thermal energy in the form of 401 kW steam and 468 kW chilled water, and had a one-hour peak load thermal storage. Evaporation ponds are shallow pools that concentrate dissolved solids through evaporation. The use of evaporation ponds to obtain salt from sea water is one of the oldest applications of solar energy. Modern uses include concentrating brine solutions used in leach mining and removing dissolved solids from waste\",\n",
        "    \"The Bush administration then turned its attention to Iraq, and argued the need to remove Saddam Hussein from power in Iraq had become urgent. Among the stated reasons were that Saddam's regime had tried to acquire nuclear material and had not properly accounted for biological and chemical material it was known to have previously possessed, and believed to still maintain. Both the possession of these weapons of mass destruction (WMD), and the failure to account for them, would violate the U.N. sanctions. The assertion about WMD was hotly advanced by the Bush administration from the beginning, but other major powers including China, France, Germany, and Russia remained unconvinced that Iraq was a threat and refused to allow passage of a UN Security Council resolution to authorize the use of force. Iraq permitted UN weapon inspectors in November 2002, who were continuing their work to assess the WMD claim when the Bush administration decided to proceed with war without UN authorization and told the inspectors to leave the\"\n",
        "] # you should replace the human texts with the text in your human corpus\n",
        "# get prefix\n",
        "prefix_input_ids=generator_tokenizer([f\"{instruction} {item}\" for item in Human_texts],max_length=30,padding='max_length',truncation=True,return_tensors=\"pt\")\n",
        "prefix_input_ids={k:v.to(device) for k,v in prefix_input_ids.items()}\n",
        "# generate\n",
        "outputs = generator.generate(\n",
        "    **prefix_input_ids,\n",
        "    max_new_tokens = 512,\n",
        "    do_sample = True,\n",
        "    temperature = 0.6,\n",
        "    top_p = 0.9,\n",
        "    pad_token_id=generator_tokenizer.pad_token_id\n",
        ")\n",
        "output_text = generator_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "# remove the instruction\n",
        "AI_texts=[\n",
        "    item.replace(\"You are helpful assistant to complete given text: \",\"\") for item in output_text\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Paraphrase AI-Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ea301b/anaconda3/envs/binh_mamba/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Use trained paraphraser\n",
        "# IF you want to use a model spefifically trained for paraphrsing, you can implement the paraphrasing pocess like the code below\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\")\n",
        "model = transformers.AutoModelForSeq2SeqLM.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\").to(device)\n",
        "\n",
        "def _model_paraphrase(\n",
        "    question,\n",
        "    num_beams=5,\n",
        "    num_beam_groups=5,\n",
        "    num_return_sequences=1,\n",
        "    repetition_penalty=10.0,\n",
        "    diversity_penalty=3.0,\n",
        "    no_repeat_ngram_size=2,\n",
        "    temperature=0.7,\n",
        "    max_length=512\n",
        "):\n",
        "    input_ids = tokenizer(\n",
        "        f'paraphrase: {question}',\n",
        "        return_tensors=\"pt\", padding=\"longest\",\n",
        "        max_length=max_length,\n",
        "        truncation=True,\n",
        "    ).input_ids\n",
        "    \n",
        "    outputs = model.generate(\n",
        "        input_ids.to(device), temperature=temperature, repetition_penalty=repetition_penalty,\n",
        "        num_return_sequences=num_return_sequences, no_repeat_ngram_size=no_repeat_ngram_size,\n",
        "        num_beams=num_beams, num_beam_groups=num_beam_groups,\n",
        "        max_length=max_length, diversity_penalty=diversity_penalty\n",
        "    )\n",
        "\n",
        "    res = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "    return res\n",
        "\n",
        "Paraphrased_ai_text=[_model_paraphrase(item) for item in AI_texts]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# We suggest to use gpt-3.5-turbo/ gpt-4 as the paraphraser\n",
        "# Which is the Unseen paraphraser mentioned in the paper\n",
        "import openai\n",
        "openai.api_key = \"your_api_key\"\n",
        "def _openai_response(text,openai_model):\n",
        "    # get paraphrases from the openai model\n",
        "    system_instruct = {\"role\": \"system\", \"content\": \"Enhance the word choices in the sentence to sound more like that of a human.\"}\n",
        "    user_input={\"role\": \"user\", \"content\": text}\n",
        "    messages = [system_instruct,user_input]\n",
        "    k_wargs = { \"messages\":messages, \"model\": openai_model}\n",
        "    r = openai.ChatCompletion.create(**k_wargs)['choices'][0].message.content\n",
        "    return r \n",
        "\n",
        "Paraphrased_ai_text=[_openai_response(item,\"gpt-3.5-turbo\") for item in AI_texts]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlVWEymLRhTl",
        "outputId": "20535e5f-a03e-4a80-db2f-4ac93665c34f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 3 input instances\n",
            "Probability of AI-generated texts is [0.9988480806350708, 0.9989444017410278, 0.9992052912712097]\n"
          ]
        }
      ],
      "source": [
        "Text_input = Human_texts\n",
        "# Use detector to deternine wehther the text_input is ai-generated.\n",
        "with torch.no_grad():\n",
        "  inputs = tokenizer(Text_input, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "  inputs = {k:v.to(device) for k,v in inputs.items()}\n",
        "  output_probs = F.log_softmax(detector(**inputs).logits,-1)[:,0].exp().tolist()\n",
        "# output_probs is the probability that the input_text is generated by LLM.\n",
        "print(\"There are\",len(Text_input),\"input instances\")\n",
        "print(\"Probability of AI-generated texts is\",output_probs)\n",
        "human_preds=output_probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 3 input instances\n",
            "Probability of AI-generated texts is [0.9975349307060242, 0.9987163543701172, 0.9979824423789978]\n"
          ]
        }
      ],
      "source": [
        "Text_input = AI_texts\n",
        "# Use detector to deternine wehther the text_input is ai-generated.\n",
        "with torch.no_grad():\n",
        "  inputs = tokenizer(Text_input, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "  inputs = {k:v.to(device) for k,v in inputs.items()}\n",
        "  output_probs = F.log_softmax(detector(**inputs).logits,-1)[:,0].exp().tolist()\n",
        "# output_probs is the probability that the input_text is generated by LLM.\n",
        "print(\"There are\",len(Text_input),\"input instances\")\n",
        "print(\"Probability of AI-generated texts is\",output_probs)\n",
        "ai_preds=output_probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['This is a hypothetical scenario where Maj Richard Scott, 40, is accused of driving erratically and causing the death of Judith Johnson, 24. However, it seems that the text is incomplete as it only includes specific dates, types of vehicles, and locations from official sources. Please consider amending or revise the information provided in this draft narrative to provide more precise details about the facts involved.'],\n",
              " ['Solar concentrating technologies can be applied in a variety of ways, including parabolic dish, trough, and Scheffler reflectors, which provide process heat for industrial applications. They can also be used to heat water using traditional methods such as solar thermal power plants or agricultural processes. This type of energy systems can improve efficiency and reduce conventional energy costs by combining the use of different sources.'],\n",
              " [\"Following that, the Bush administration shifted its focus to Iraq and advocated for the removal of Saddam Hussein from power to restore stability in the Middle East. The US-led coalition invaded Iraq in March 2003, but this decision was heavily criticized due to allegations of WMDs and links between Husain's regime and Al-Qaeda, which led to an increase in violence and civilian casualties.\"]]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Text_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 3 input instances\n",
            "Probability of AI-generated texts is [0.9992269277572632, 0.999092698097229, 0.9994516968727112]\n"
          ]
        }
      ],
      "source": [
        "Text_input = [item[0] for item in Paraphrased_ai_text]\n",
        "# Use detector to deternine wehther the text_input is ai-generated.\n",
        "with torch.no_grad():\n",
        "  inputs = tokenizer(Text_input, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "  inputs = {k:v.to(device) for k,v in inputs.items()}\n",
        "  output_probs = F.log_softmax(detector(**inputs).logits,-1)[:,0].exp().tolist()\n",
        "# output_probs is the probability that the input_text is generated by LLM.\n",
        "print(\"There are\",len(Text_input),\"input instances\")\n",
        "print(\"Probability of AI-generated texts is\",output_probs)\n",
        "paraphrased_ai_preds=output_probs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Computing Detection AUROC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import auc,roc_curve\n",
        "def get_roc_metrics(human_preds, ai_preds):\n",
        "    # human_preds is the ai-generated probabiities of human-text\n",
        "    # ai_preds is the ai-generated probabiities of ai-text\n",
        "    fpr, tpr, _ = roc_curve([0] * len(human_preds) + [1] * len(ai_preds), human_preds + ai_preds,pos_label=1)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    return fpr.tolist(), tpr.tolist(), float(roc_auc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "W/O Paraphrase Detection AUROC:  ([0.0, 0.3333333333333333, 1.0, 1.0], [0.0, 0.0, 0.0, 1.0], 0.0)\n",
            "W/ Paraphrase Detection AUROC:  ([0.0, 0.0, 0.0, 0.3333333333333333, 0.3333333333333333, 1.0], [0.0, 0.3333333333333333, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0], 0.888888888888889)\n"
          ]
        }
      ],
      "source": [
        "print(\"W/O Paraphrase Detection AUROC: \", get_roc_metrics(human_preds,ai_preds))\n",
        "print(\"W/ Paraphrase Detection AUROC: \", get_roc_metrics(human_preds,paraphrased_ai_preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Use specifically trained paraphraser to paraphrase"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "binh_mamba",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
