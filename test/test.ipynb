{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ea301b/anaconda3/envs/binh_mamba/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/ea301b/anaconda3/envs/binh_mamba/lib/python3.12/site-packages/mamba_ssm/ops/selective_scan_interface.py:163: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\n",
      "/home/ea301b/anaconda3/envs/binh_mamba/lib/python3.12/site-packages/mamba_ssm/ops/selective_scan_interface.py:239: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\n",
      "/home/ea301b/anaconda3/envs/binh_mamba/lib/python3.12/site-packages/mamba_ssm/ops/triton/layer_norm.py:985: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\n",
      "/home/ea301b/anaconda3/envs/binh_mamba/lib/python3.12/site-packages/mamba_ssm/ops/triton/layer_norm.py:1044: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\n",
      "/home/ea301b/anaconda3/envs/binh_mamba/lib/python3.12/site-packages/mamba_ssm/distributed/tensor_parallel.py:25: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\n",
      "/home/ea301b/anaconda3/envs/binh_mamba/lib/python3.12/site-packages/mamba_ssm/distributed/tensor_parallel.py:61: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\n",
      "/home/ea301b/anaconda3/envs/binh_mamba/lib/python3.12/site-packages/mamba_ssm/ops/triton/ssd_combined.py:739: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\n",
      "/home/ea301b/anaconda3/envs/binh_mamba/lib/python3.12/site-packages/mamba_ssm/ops/triton/ssd_combined.py:817: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\n",
      "You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Mamba2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n",
      "Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Mamba2Model is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import torch\n",
    "# from safetensors.torch import load_file  # For loading .safetensors files\n",
    "# from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM, DataCollatorWithPadding\n",
    "# from model_sequence_classification import CustomModelForSequenceClassification\n",
    "\n",
    "# # Step 1: Load configuration and tokenizer\n",
    "# checkpoint_path = \"binh230/mambaformer_ver1\"  # Update to your Hugging Face model repo\n",
    "# config = AutoConfig.from_pretrained(checkpoint_path, trust_remote_code=True)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(checkpoint_path, trust_remote_code=True)\n",
    "# data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# # Step 2: Initialize the base model\n",
    "# base_model_1 = AutoModelForCausalLM.from_pretrained(\n",
    "#     \"OuteAI/Lite-Oute-2-Mamba2Attn-Base\",\n",
    "#     config=config,\n",
    "#     trust_remote_code=True,\n",
    "#     attn_implementation=\"flash_attention_2\",\n",
    "# )\n",
    "\n",
    "\n",
    "# # Load the state dict from the checkpoint\n",
    "# weights_path = f\"/home/HardDisk/binh230_intern/Mamba-AI-generated-text-detection/train/finetune_model/results/mambaformer/model (3).safetensors\"\n",
    "# state_dict = load_file(weights_path)\n",
    "\n",
    "# # Step 3: Wrap each base model in the custom classification model and load the state dict\n",
    "# model = CustomModelForSequenceClassification(config, base_model_1)\n",
    "# model.load_state_dict(state_dict, strict=False)  # Set strict=False if there are missing keys\n",
    "# # Now both models are ready for further processing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import numpy as np\n",
    "# from safetensors.torch import load_file  # For loading .safetensors files\n",
    "# from transformers import DataCollatorWithPadding, AutoConfig, AutoTokenizer\n",
    "# from model_sequence_classification import CustomModelForSequenceClassification\n",
    "# from mamba_ssm import MambaLMHeadModel\n",
    "# # Dataset and Tokenizer Setup\n",
    "# # data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# # Load the model\n",
    "# FOUNDATION_MODEL_NAME = \"state-spaces/mamba2-370m\"\n",
    "# model1 = MambaLMHeadModel.from_pretrained(FOUNDATION_MODEL_NAME)\n",
    "# # model.lm_head = torch.nn.Linear(model.config.d_model, 2)\n",
    "# # model = nn.DataParallel(model)\n",
    "# tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neox-20b')\n",
    "# config = AutoConfig.from_pretrained(FOUNDATION_MODEL_NAME, trust_remote_code=True,)\n",
    "# model = CustomModelForSequenceClassification(config, model1)\n",
    "# weights_path = f\"/home/HardDisk/binh230_intern/Mamba-AI-generated-text-detection/test/results/mamba/model (2).safetensors\"\n",
    "# state_dict = load_file(weights_path)\n",
    "# model1.load_state_dict(state_dict, strict=False)  # Set strict=False if there are missing keys\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoConfig, AutoTokenizer, DataCollatorWithPadding\n",
    "from transformers import DebertaV2ForSequenceClassification\n",
    "\n",
    "# Load the tokenizer for the DeBERTa model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n",
    "\n",
    "# Dataset and Tokenizer Setup\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Model Configuration Setup with AutoConfig\n",
    "# This will check for the config in the model repository and allow customization if needed\n",
    "config = AutoConfig.from_pretrained(\"microsoft/deberta-v3-base\")\n",
    "config.num_labels = 2  # Set number of labels as per your requirement\n",
    "\n",
    "# Initialize the model with sequence classification head and config\n",
    "model = DebertaV2ForSequenceClassification.from_pretrained(\"microsoft/deberta-v3-base\", config=config)\n",
    "# model.to(\"cuda\")\n",
    "\n",
    "# (Optional) Print model summary to verify setup\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "\n",
    "json_dataset = load_dataset(\"json\", data_files=\"/home/HardDisk/binh230_intern/Mamba-AI-generated-text-detection/test/exp_gpt3to4/data/pubmed_gpt-4.raw_data.json\")\n",
    "\n",
    "# Create a new list to store the converted data\n",
    "new_data = {'text': [], 'labels': []}\n",
    "train_data = {'text': [], 'labels': []}\n",
    "# Convert 'original' texts to (text, label) format with label 0, and 'sampled' with label 1\n",
    "for row in json_dataset['train']:\n",
    "    for i in range(150):\n",
    "        if i == 0:\n",
    "            train_data['text'].append(row['original'][i])\n",
    "            train_data['labels'].append(0)  # Label for 'original' text \n",
    "        new_data['text'].append(row['original'][i])\n",
    "        new_data['labels'].append(0)  # Label for 'original' text\n",
    "        new_data['text'].append(row['sampled'][i])\n",
    "        new_data['labels'].append(1)  # Label for 'sampled' text\n",
    "\n",
    "# Convert to a new Dataset with 'text' and 'labels' features\n",
    "new_dataset = DatasetDict({\n",
    "    'valid': Dataset.from_dict(new_data),\n",
    "    'train': Dataset.from_dict(train_data)\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 3266.50 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 121.84 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import DataCollatorWithPadding, AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "from model_sequence_classification import CustomModelForSequenceClassification\n",
    "from mamba_ssm import MambaLMHeadModel\n",
    "\n",
    "# Add eos tokens\n",
    "# tokenizer.eos_token = \"<|endoftext|>\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # Tokenize the text with truncation\n",
    "    samples = tokenizer(examples['text'], \n",
    "                        truncation=True, \n",
    "                        padding='max_length', \n",
    "                        max_length=512,         \n",
    "                        return_tensors=\"pt\")\n",
    "    \n",
    "    return samples\n",
    "\n",
    "# Apply preprocessing to the dataset\n",
    "tokenized_dataset = new_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/HardDisk/binh230_intern/transformers_zamba2/src/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/ea301b/anaconda3/envs/binh_mamba/lib/python3.12/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [75/75 00:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ea301b/anaconda3/envs/binh_mamba/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtruonggiabjnh2003\u001b[0m (\u001b[33mtruonggiabjnh2003-fpt-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/HardDisk/binh230_intern/Mamba-AI-generated-text-detection/test/wandb/run-20241029_044608-khchmow7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/truonggiabjnh2003-fpt-university/huggingface/runs/khchmow7' target=\"_blank\">./results/mamba</a></strong> to <a href='https://wandb.ai/truonggiabjnh2003-fpt-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/truonggiabjnh2003-fpt-university/huggingface' target=\"_blank\">https://wandb.ai/truonggiabjnh2003-fpt-university/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/truonggiabjnh2003-fpt-university/huggingface/runs/khchmow7' target=\"_blank\">https://wandb.ai/truonggiabjnh2003-fpt-university/huggingface/runs/khchmow7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.739332675933838,\n",
       " 'eval_model_preparation_time': 0.0047,\n",
       " 'eval_accuracy': 0.5,\n",
       " 'eval_precision': 0.25,\n",
       " 'eval_recall': 0.5,\n",
       " 'eval_f1': 0.3333333333333333,\n",
       " 'eval_auroc': 0.6119111111111111,\n",
       " 'eval_runtime': 19.6807,\n",
       " 'eval_samples_per_second': 15.243,\n",
       " 'eval_steps_per_second': 3.811}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import wandb  # Weights & Biases integration\n",
    "from torch import nn\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "from typing import Dict, Union\n",
    "import torch\n",
    "from transformers import (\n",
    "    DataCollatorWithPadding, \n",
    "    AdamW, \n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    "    get_cosine_schedule_with_warmup,\n",
    "    TrainerCallback\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "from huggingface_hub import login  # For pushing to the Hugging Face Hub\n",
    "\n",
    "# Authenticate Hugging Face API token\n",
    "# Make sure you've logged in before running the script\n",
    "# login(token=\"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\")\n",
    "# Initialize wandb run\n",
    "\n",
    "\n",
    "\n",
    "# Data Collator Setup\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Compute metrics for Hugging Face Trainer, including AUROC.\n",
    "\n",
    "    Args:\n",
    "        eval_pred: tuple of (predictions, labels) where predictions are logits.\n",
    "\n",
    "    Returns:\n",
    "        dictionary containing the computed metrics, including AUROC.\n",
    "    \"\"\"\n",
    "    # Unpack predictions and labels\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(-1)  # Get the predicted class\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "\n",
    "    # Calculate precision, recall, and F1-score\n",
    "    precision = precision_score(labels, preds, average='weighted')\n",
    "    recall = recall_score(labels, preds, average='weighted')\n",
    "    f1 = f1_score(labels, preds, average='weighted')\n",
    "\n",
    "    # Calculate probabilities using softmax on logits (not on preds)\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=-1).numpy()\n",
    "    # For binary classification, take the probability of the positive class (class 1)\n",
    "    auroc = roc_auc_score(labels, probs[:, 1])\n",
    "\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'auroc': auroc\n",
    "    }\n",
    "\n",
    "# Training Arguments Setup\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results/mamba\",  # Directory to save model checkpoints\n",
    "    evaluation_strategy=\"steps\",  # Evaluate every few steps\n",
    "    # eval_steps=1024,  # Evaluate every 1000 steps\n",
    "    per_device_eval_batch_size=4,  # Same for evaluation\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=1,  # Define total number of epochs\n",
    "    weight_decay=0.1,  # L2 regularization\n",
    "    logging_dir=\"./logs\",  # Log directory\n",
    "    logging_steps=256,\n",
    "    fp16=True,  # Use mixed precision training\n",
    "    save_steps=1000,  # Save model every 2000 steps\n",
    "    label_smoothing_factor=0.03,\n",
    "    # hub_model_id=\"mambaformer_ver1\",  # Set model name for HF Hub\n",
    "    # push_to_hub=True,  # Push to Hugging Face Hub\n",
    "    # save_total_limit=2,  # Only keep the last 2 checkpoints,\n",
    "    metric_for_best_model=\"eval_auroc\",  # Use AUROC to determine best model\n",
    "    greater_is_better=True,         # Higher AUROC is better\n",
    "    max_grad_norm=1,\n",
    "    # report_to=\"wandb\",               # Report metrics to Weights & Biases\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],  # Replace with your actual training dataset\n",
    "    eval_dataset=tokenized_dataset['valid'],    # Replace with your actual evaluation dataset\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics  # Optional custom metric computation\n",
    ")\n",
    "\n",
    "# Training and evaluation\n",
    "# trainer.train()\n",
    "trainer.evaluate()\n",
    "\n",
    "# Push to Hub\n",
    "# trainer.save_model()  # Specify the directory where you want to save\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid decimal literal (241441669.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[6], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    Mamba2-370m\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid decimal literal\n"
     ]
    }
   ],
   "source": [
    "            writing_gpt, xsum, pub_med\n",
    "Deberta    0.97, 0.9, 0.8311333333333334\n",
    "Mamba2-370m 0.92\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "binh_mamba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
